sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544197357_614961/predict.txt | sacrebleu ../eval/1544197357_614961/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 10.9 38.9/14.5/7.2/3.7 (BP = 0.981 ratio = 0.982 hyp_len = 28705 ref_len = 29243)\n')
epoch:  0  the blue score on validation dataset is :  10.9
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544238928_7019646/predict.txt | sacrebleu ../eval/1544238928_7019646/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 12.8 41.3/16.6/8.4/4.6 (BP = 1.000 ratio = 1.013 hyp_len = 29613 ref_len = 29243)\n')
epoch:  1  the blue score on validation dataset is :  12.8
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544280738_2469318/predict.txt | sacrebleu ../eval/1544280738_2469318/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 13.7 42.3/17.8/9.2/5.1 (BP = 1.000 ratio = 1.034 hyp_len = 30243 ref_len = 29243)\n')
epoch:  2  the blue score on validation dataset is :  13.7
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544322668_3744583/predict.txt | sacrebleu ../eval/1544322668_3744583/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 15.0 47.1/20.4/10.9/6.1 (BP = 0.946 ratio = 0.947 hyp_len = 27695 ref_len = 29243)\n')
epoch:  3  the blue score on validation dataset is :  15.0
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544364416_499185/predict.txt | sacrebleu ../eval/1544364416_499185/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 15.3 46.7/20.3/10.8/6.1 (BP = 0.972 ratio = 0.973 hyp_len = 28440 ref_len = 29243)\n')
epoch:  4  the blue score on validation dataset is :  15.3
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544406180_2545469/predict.txt | sacrebleu ../eval/1544406180_2545469/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 14.7 47.9/20.8/11.0/6.1 (BP = 0.912 ratio = 0.916 hyp_len = 26774 ref_len = 29243)\n')
epoch:  5  the blue score on validation dataset is :  14.7
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544447932_9919875/predict.txt | sacrebleu ../eval/1544447932_9919875/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 15.6 46.9/20.7/11.1/6.4 (BP = 0.962 ratio = 0.963 hyp_len = 28160 ref_len = 29243)\n')
epoch:  6  the blue score on validation dataset is :  15.6
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544489701_0707514/predict.txt | sacrebleu ../eval/1544489701_0707514/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 15.3 46.8/20.5/10.9/6.1 (BP = 0.960 ratio = 0.961 hyp_len = 28109 ref_len = 29243)\n')
epoch:  7  the blue score on validation dataset is :  15.3
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544531451_2602847/predict.txt | sacrebleu ../eval/1544531451_2602847/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 15.5 47.3/20.8/11.0/6.1 (BP = 0.966 ratio = 0.967 hyp_len = 28271 ref_len = 29243)\n')
epoch:  8  the blue score on validation dataset is :  15.5
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544573060_752429/predict.txt | sacrebleu ../eval/1544573060_752429/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 15.0 48.3/21.1/11.2/6.3 (BP = 0.916 ratio = 0.919 hyp_len = 26876 ref_len = 29243)\n')
epoch:  9  the blue score on validation dataset is :  15.0
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544614855_77726/predict.txt | sacrebleu ../eval/1544614855_77726/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 15.5 48.6/21.5/11.5/6.7 (BP = 0.919 ratio = 0.922 hyp_len = 26966 ref_len = 29243)\n')
epoch:  10  the blue score on validation dataset is :  15.5
--------------------------------------
./train_better_teacher.py:92: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  output, (h_n, c_n) = self.lstm(X_data, (h, c))
./train_better_teacher.py:172: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  output, (h_t, c_t) = self.lstm(input_vector,(hidden_vector, cell_vector))
Traceback (most recent call last):
  File "./train_better_teacher.py", line 479, in <module>
    train(encoder, decoder, optimizer, train_zh_en_iter, teacher_forcing_ratio)
  File "./train_better_teacher.py", line 228, in train
    prob, h_t, c_t = decoder(target_word, h_t, c_t, output, is_init)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "./train_better_teacher.py", line 172, in forward
    output, (h_t, c_t) = self.lstm(input_vector,(hidden_vector, cell_vector))
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 192, in forward
    output, hidden = func(input, self.all_weights, hx, batch_sizes)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py", line 324, in forward
    return func(input, *fargs, **fkwargs)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py", line 288, in forward
    dropout_ts)
RuntimeError: CUDA error: out of memory
