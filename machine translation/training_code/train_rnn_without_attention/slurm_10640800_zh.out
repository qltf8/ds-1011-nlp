sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1543900836_1285212/predict.txt | sacrebleu ../eval/1543900836_1285212/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 8.6 32.8/11.4/5.4/2.8 (BP = 1.000 ratio = 1.056 hyp_len = 30890 ref_len = 29243)\n')
epoch:  0  the blue score on validation dataset is :  8.6
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1543933328_2229145/predict.txt | sacrebleu ../eval/1543933328_2229145/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 9.2 34.3/12.2/5.8/2.9 (BP = 1.000 ratio = 1.068 hyp_len = 31242 ref_len = 29243)\n')
epoch:  1  the blue score on validation dataset is :  9.2
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1543965762_9967544/predict.txt | sacrebleu ../eval/1543965762_9967544/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 9.2 35.4/12.4/5.7/2.9 (BP = 1.000 ratio = 1.014 hyp_len = 29667 ref_len = 29243)\n')
epoch:  2  the blue score on validation dataset is :  9.2
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1543998264_5972817/predict.txt | sacrebleu ../eval/1543998264_5972817/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 9.7 35.9/12.9/6.1/3.2 (BP = 1.000 ratio = 1.016 hyp_len = 29714 ref_len = 29243)\n')
epoch:  3  the blue score on validation dataset is :  9.7
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544030842_4295268/predict.txt | sacrebleu ../eval/1544030842_4295268/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 9.7 38.3/13.3/6.3/3.2 (BP = 0.965 ratio = 0.966 hyp_len = 28236 ref_len = 29243)\n')
epoch:  4  the blue score on validation dataset is :  9.7
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544063428_2720914/predict.txt | sacrebleu ../eval/1544063428_2720914/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 9.6 37.6/13.1/6.1/3.1 (BP = 0.981 ratio = 0.981 hyp_len = 28679 ref_len = 29243)\n')
epoch:  5  the blue score on validation dataset is :  9.6
sacreBLEU: That's 100 lines that end in a tokenized period ('.')
sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.
sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
CompletedProcess(args='cat ../eval/1544096031_0209084/predict.txt | sacrebleu ../eval/1544096031_0209084/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 9.6 37.2/12.9/5.9/3.1 (BP = 0.992 ratio = 0.992 hyp_len = 28998 ref_len = 29243)\n')
epoch:  6  the blue score on validation dataset is :  9.6
--------------------------------------
./train_better_teacher.py:88: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  output, (h_n, c_n) = self.lstm(X_data, (h, c))
./train_better_teacher.py:159: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().
  output, (h_t, c_t) = self.lstm(input_vector,(hidden_vector, cell_vector))
Traceback (most recent call last):
  File "./train_better_teacher.py", line 466, in <module>
    train(encoder, decoder, optimizer, train_vi_en_iter, teacher_forcing_ratio)
  File "./train_better_teacher.py", line 215, in train
    prob, h_t, c_t = decoder(target_word, h_t, c_t, output, is_init)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "./train_better_teacher.py", line 159, in forward
    output, (h_t, c_t) = self.lstm(input_vector,(hidden_vector, cell_vector))
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 192, in forward
    output, hidden = func(input, self.all_weights, hx, batch_sizes)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py", line 324, in forward
    return func(input, *fargs, **fkwargs)
  File "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py", line 288, in forward
    dropout_ts)
RuntimeError: CUDA error: out of memory
