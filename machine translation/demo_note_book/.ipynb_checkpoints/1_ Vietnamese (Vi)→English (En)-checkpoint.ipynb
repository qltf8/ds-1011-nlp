{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM ensember beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 512])\n",
      "torch.Size([1, 15, 512]) torch.Size([15, 512])\n",
      "torch.Size([1, 28, 512])\n",
      "torch.Size([1, 28, 512]) torch.Size([28, 512])\n",
      "torch.Size([1, 11, 512])\n",
      "torch.Size([1, 11, 512]) torch.Size([11, 512])\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 512]) torch.Size([8, 512])\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 512]) torch.Size([5, 512])\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 512]) torch.Size([5, 512])\n",
      "torch.Size([1, 12, 512])\n",
      "torch.Size([1, 12, 512]) torch.Size([12, 512])\n",
      "torch.Size([1, 7, 512])\n",
      "torch.Size([1, 7, 512]) torch.Size([7, 512])\n",
      "torch.Size([1, 39, 512])\n",
      "torch.Size([1, 39, 512]) torch.Size([39, 512])\n",
      "torch.Size([1, 32, 512])\n",
      "torch.Size([1, 32, 512]) torch.Size([32, 512])\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 512]) torch.Size([8, 512])\n",
      "torch.Size([1, 23, 512])\n",
      "torch.Size([1, 23, 512]) torch.Size([23, 512])\n",
      "torch.Size([1, 9, 512])\n",
      "torch.Size([1, 9, 512]) torch.Size([9, 512])\n",
      "torch.Size([1, 22, 512])\n",
      "torch.Size([1, 22, 512]) torch.Size([22, 512])\n",
      "torch.Size([1, 22, 512])\n",
      "torch.Size([1, 22, 512]) torch.Size([22, 512])\n",
      "torch.Size([1, 16, 512])\n",
      "torch.Size([1, 16, 512]) torch.Size([16, 512])\n",
      "torch.Size([1, 12, 512])\n",
      "torch.Size([1, 12, 512]) torch.Size([12, 512])\n",
      "torch.Size([1, 38, 512])\n",
      "torch.Size([1, 38, 512]) torch.Size([38, 512])\n",
      "torch.Size([1, 22, 512])\n",
      "torch.Size([1, 22, 512]) torch.Size([22, 512])\n",
      "torch.Size([1, 11, 512])\n",
      "torch.Size([1, 11, 512]) torch.Size([11, 512])\n",
      "torch.Size([1, 23, 512])\n",
      "torch.Size([1, 23, 512]) torch.Size([23, 512])\n",
      "torch.Size([1, 42, 512])\n",
      "torch.Size([1, 42, 512]) torch.Size([42, 512])\n",
      "torch.Size([1, 16, 512])\n",
      "torch.Size([1, 16, 512]) torch.Size([16, 512])\n",
      "torch.Size([1, 7, 512])\n",
      "torch.Size([1, 7, 512]) torch.Size([7, 512])\n",
      "torch.Size([1, 22, 512])\n",
      "torch.Size([1, 22, 512]) torch.Size([22, 512])\n",
      "torch.Size([1, 7, 512])\n",
      "torch.Size([1, 7, 512]) torch.Size([7, 512])\n",
      "torch.Size([1, 18, 512])\n",
      "torch.Size([1, 18, 512]) torch.Size([18, 512])\n",
      "torch.Size([1, 15, 512])\n",
      "torch.Size([1, 15, 512]) torch.Size([15, 512])\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 512]) torch.Size([5, 512])\n",
      "torch.Size([1, 32, 512])\n",
      "torch.Size([1, 32, 512]) torch.Size([32, 512])\n",
      "torch.Size([1, 61, 512])\n",
      "torch.Size([1, 61, 512]) torch.Size([61, 512])\n",
      "torch.Size([1, 6, 512])\n",
      "torch.Size([1, 6, 512]) torch.Size([6, 512])\n",
      "torch.Size([1, 29, 512])\n",
      "torch.Size([1, 29, 512]) torch.Size([29, 512])\n",
      "torch.Size([1, 31, 512])\n",
      "torch.Size([1, 31, 512]) torch.Size([31, 512])\n",
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 512]) torch.Size([10, 512])\n",
      "torch.Size([1, 6, 512])\n",
      "torch.Size([1, 6, 512]) torch.Size([6, 512])\n",
      "torch.Size([1, 24, 512])\n",
      "torch.Size([1, 24, 512]) torch.Size([24, 512])\n",
      "torch.Size([1, 13, 512])\n",
      "torch.Size([1, 13, 512]) torch.Size([13, 512])\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 512]) torch.Size([8, 512])\n",
      "torch.Size([1, 18, 512])\n",
      "torch.Size([1, 18, 512]) torch.Size([18, 512])\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 512]) torch.Size([8, 512])\n",
      "torch.Size([1, 33, 512])\n",
      "torch.Size([1, 33, 512]) torch.Size([33, 512])\n",
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 512]) torch.Size([10, 512])\n",
      "torch.Size([1, 51, 512])\n",
      "torch.Size([1, 51, 512]) torch.Size([51, 512])\n",
      "torch.Size([1, 28, 512])\n",
      "torch.Size([1, 28, 512]) torch.Size([28, 512])\n",
      "torch.Size([1, 26, 512])\n",
      "torch.Size([1, 26, 512]) torch.Size([26, 512])\n",
      "torch.Size([1, 8, 512])\n",
      "torch.Size([1, 8, 512]) torch.Size([8, 512])\n",
      "torch.Size([1, 12, 512])\n",
      "torch.Size([1, 12, 512]) torch.Size([12, 512])\n",
      "torch.Size([1, 6, 512])\n",
      "torch.Size([1, 6, 512]) torch.Size([6, 512])\n",
      "torch.Size([1, 22, 512])\n",
      "torch.Size([1, 22, 512]) torch.Size([22, 512])\n",
      "torch.Size([1, 21, 512])\n",
      "torch.Size([1, 21, 512]) torch.Size([21, 512])\n",
      "torch.Size([1, 27, 512])\n",
      "torch.Size([1, 27, 512]) torch.Size([27, 512])\n",
      "torch.Size([1, 17, 512])\n",
      "torch.Size([1, 17, 512]) torch.Size([17, 512])\n",
      "torch.Size([1, 19, 512])\n",
      "torch.Size([1, 19, 512]) torch.Size([19, 512])\n",
      "torch.Size([1, 28, 512])\n",
      "torch.Size([1, 28, 512]) torch.Size([28, 512])\n",
      "torch.Size([1, 18, 512])\n",
      "torch.Size([1, 18, 512]) torch.Size([18, 512])\n",
      "torch.Size([1, 87, 512])\n",
      "torch.Size([1, 87, 512]) torch.Size([87, 512])\n",
      "torch.Size([1, 33, 512])\n",
      "torch.Size([1, 33, 512]) torch.Size([33, 512])\n",
      "torch.Size([1, 22, 512])\n",
      "torch.Size([1, 22, 512]) torch.Size([22, 512])\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 512]) torch.Size([5, 512])\n",
      "torch.Size([1, 18, 512])\n",
      "torch.Size([1, 18, 512]) torch.Size([18, 512])\n",
      "torch.Size([1, 13, 512])\n",
      "torch.Size([1, 13, 512]) torch.Size([13, 512])\n",
      "torch.Size([1, 18, 512])\n",
      "torch.Size([1, 18, 512]) torch.Size([18, 512])\n",
      "torch.Size([1, 4, 512])\n",
      "torch.Size([1, 4, 512]) torch.Size([4, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-945776af2646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex_unique\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_vi_en_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0mblue_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_vi_en_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' the blue score on validation dataset is : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblue_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-945776af2646>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, optimizer, data_iter, teacher_forcing_ratio, batch_size)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, sys, os\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import csv\n",
    "import re, random, string, subprocess, time\n",
    "\n",
    "\n",
    "TEXT_vi = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = True, tokenize= lambda t:t.split(),\n",
    "                                        include_lengths=True)\n",
    "TEXT_en = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = False, tokenize= lambda t:t.split(),\n",
    "                              lower=True, init_token='<sos>', eos_token='<eos>',include_lengths=True)\n",
    "train_vi_en = torchtext.data.TabularDataset('/home/ql819/text_data/train_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "validation_vi_en = torchtext.data.TabularDataset('/home/ql819/text_data/dev_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "\n",
    "\n",
    "TEXT_vi.build_vocab(train_vi_en, min_freq=3)\n",
    "TEXT_en.build_vocab(train_vi_en, min_freq=3)\n",
    "\n",
    "train_vi_en_iter = torchtext.data.BucketIterator(train_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "\n",
    "class GRU_Decoder_With_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 1\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(self.num_vocab, self.input_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size= self.hidden_size, input_size= self.input_size + 1 * self.hidden_size, \n",
    "                                  num_layers= self.num_layers)\n",
    "        \n",
    "        self.calcu_weight_1  = torch.nn.Linear(2*self.hidden_size, hidden_size)\n",
    "        self.calcu_weight_2  = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.init_weight = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.linear_vob = torch.nn.Linear(self.hidden_size, self.num_vocab)\n",
    "        \n",
    "    def forward(self, input_word_index, hidden_vector, encoder_memory, is_init = False):\n",
    "        #input_word_index: [num]\n",
    "        #hidden_vector: 1, 1, hidden_size\n",
    "        #encoder_memory: source_sen_len , 1 * hidden_size\n",
    "        \n",
    "        if hidden_vector.shape[0] != self.num_layers or hidden_vector.shape[2] != self.hidden_size:\n",
    "            raise ValueError('The size of hidden_vector is not correct, expect '+str((self.num_layers, self.hidden_size))\\\n",
    "                            + ', actually get ' + str(hidden_vector.shape))\n",
    "        \n",
    "        if is_init:\n",
    "            hidden_vector = torch.tanh(self.init_weight(hidden_vector))\n",
    "        \n",
    "        \n",
    "        n_hidden_vector = torch.stack([hidden_vector.squeeze()]*encoder_memory.shape[0],dim=0)\n",
    "        com_n_h_memory = torch.cat([n_hidden_vector, encoder_memory], dim =1)\n",
    "        com_n_h_temp = torch.tanh(self.calcu_weight_1(com_n_h_memory))\n",
    "        \n",
    "        \n",
    "        weight_vector = self.calcu_weight_2(com_n_h_temp)\n",
    "        weight_vector =  torch.nn.functional.softmax(weight_vector, dim=0)\n",
    "        #weight_vector: source_sen_len * 1\n",
    "        \n",
    "        \n",
    "        convect_vector = torch.mm(weight_vector.transpose(1,0), encoder_memory)\n",
    "        #convect_vector: 1 , 2 * hidden_size\n",
    "        \n",
    "        \n",
    "        input_vector = self.embedding_layer(input_word_index).view(1,1,-1)\n",
    "        \n",
    "        \n",
    "        input_vector = torch.cat([convect_vector.unsqueeze(0), input_vector], dim=2)\n",
    "        \n",
    "        \n",
    "        output, h_t = self.gru(input_vector,hidden_vector)\n",
    "        output = output.view(1, self.hidden_size)\n",
    "        \n",
    "        \n",
    "        prob = self.linear_vob(output)\n",
    "        #prob 1, vob_size\n",
    "        \n",
    "        prob = torch.nn.functional.log_softmax(prob, dim=1)\n",
    "        \n",
    "        \n",
    "        return prob, h_t\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, src_embed, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_embed = src_embed\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        x = self.src_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "\n",
    "    \n",
    "def attention(query, key, value, dropout=None):\n",
    "    '''\n",
    "    query: batch, seq1, d_k\n",
    "    key: batch, seq2, d_k\n",
    "    value: batch, seq2, embedding_size\n",
    "    mask: batch, 1, seq_2\n",
    "    '''\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_k\n",
    "        self.linears = clones(nn.Linear(d_model, d_k), 2)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        \"Implements Figure 2\"\n",
    "        \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key = [l(x) for l, x in zip(self.linears, (query, key))]\n",
    "        #query, key = batch, seq, d_k\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value,\n",
    "                                 dropout=self.dropout)\n",
    "        #x: batch, seq_query, embedding_size\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(F.relu(self.w_1(x)))\n",
    "    \n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "    \n",
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_k=64, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(d_k, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), N)\n",
    "    decoder = GRU_Decoder_With_Attention(num_vocab = tgt_vocab, input_size = d_model, hidden_size = d_model)\n",
    "    for p in encoder.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "\n",
    "def train(encoder, decoder, optimizer, data_iter, teacher_forcing_ratio, batch_size = 64):\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    count = 0\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output = encoder(source_data)\n",
    "        #all_output: 1, source_len, embedding_size\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "\n",
    "        output = all_output[0,:]\n",
    "    \n",
    "        target_word_list = target_data.squeeze()\n",
    "        target_word = torch.tensor([TEXT_en.vocab.stoi['<sos>']]).cuda(0)\n",
    "\n",
    "        h_t = output[0,:]\n",
    "        h_t = h_t.view([1,1,-1])\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "        for word_index in range(1, target_len[0].item()):\n",
    "            prob, h_t = decoder(target_word, h_t, output, is_init)\n",
    "            is_init = False\n",
    "            if use_teacher_forcing:\n",
    "                target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, target_word)\n",
    "            else:\n",
    "                right_target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, right_target_word)\n",
    "                predict_target_word_index = prob.topk(1)[1].item()\n",
    "\n",
    "                if TEXT_en.vocab.stoi['<eos>'] == predict_target_word_index:\n",
    "                    break\n",
    "                else:\n",
    "                    target_word = torch.tensor([predict_target_word_index]).cuda(0)\n",
    "                    \n",
    "        count += 1\n",
    "        if count % batch_size == 0:\n",
    "            \n",
    "            loss = loss/batch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            count = 0\n",
    "            loss = 0\n",
    "        \n",
    "        \n",
    "    if count % batch_size != 0:\n",
    "        loss = loss/count\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "class Bean_Search_Status_Record:\n",
    "    \n",
    "    def __init__(self, h_t, predict_word_index_list, sum_log_prob):\n",
    "        self.h_t = h_t\n",
    "        self.predict_word_index_list = predict_word_index_list\n",
    "        self.sum_log_prob = sum_log_prob\n",
    "        self.avg_log_prob = 0\n",
    "        \n",
    "    \n",
    "\n",
    "def test(encoder, decoder, data_iter, k=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    path_name = '../eval/'+str(time.time()).replace('.','_')+'/'\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "    predict_file_name = path_name + 'predict.txt'\n",
    "    target_file_name = path_name + 'target_file_name.txt'\n",
    "\n",
    "    predict_file = open(predict_file_name, 'w')\n",
    "    target_file = open(target_file_name, 'w')\n",
    "\n",
    "\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output = encoder(source_data)\n",
    "        output = all_output[0,:]\n",
    "        \n",
    "        target_word = torch.tensor([TEXT_en.vocab.stoi['<sos>']]).cuda(0)\n",
    "\n",
    "        h_t = output[0,:]\n",
    "        h_t = h_t.view([1,1,-1])\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "\n",
    "        right_whole_sentence_word_index = target_data[1: target_len[0].item()-1,0]\n",
    "        right_whole_sentence_word_index = list(right_whole_sentence_word_index.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        sequences = [Bean_Search_Status_Record(h_t, predict_word_index_list = [target_word], \n",
    "                                               sum_log_prob = 0.0)]\n",
    "        \n",
    "        t = 0\n",
    "        while (t < 100):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                record = sequences[i]\n",
    "                h_t = record.h_t\n",
    "                predict_word_index_list = record.predict_word_index_list\n",
    "                sum_log_prob = record.sum_log_prob\n",
    "                target_word = predict_word_index_list[-1]\n",
    "                \n",
    "                if TEXT_en.vocab.stoi['<eos>'] != target_word:\n",
    "                \n",
    "                    prob, h_t = decoder(torch.tensor([target_word]).cuda(0), h_t, output, is_init)\n",
    "\n",
    "                    k_prob_value_list, k_word_index_list = prob.topk(k,dim=1)\n",
    "                    k_prob_value_list = k_prob_value_list.cpu().detach().squeeze().numpy()\n",
    "                    k_word_index_list = k_word_index_list.cpu().squeeze().numpy()\n",
    "                    \n",
    "                    \n",
    "                    for prob_value, word_index in zip(k_prob_value_list, k_word_index_list):\n",
    "                        prob_value = float(prob_value)\n",
    "                        word_index = int(word_index)\n",
    "                        new_record = Bean_Search_Status_Record(h_t, predict_word_index_list+[word_index], sum_log_prob+prob_value)\n",
    "                        new_record.avg_log_prob = new_record.sum_log_prob/(len(new_record.predict_word_index_list) - 1)\n",
    "                        all_candidates.append(new_record)\n",
    "                else:\n",
    "                    all_candidates.append(record)\n",
    "            is_init = False\n",
    "                        \n",
    "            ordered = sorted(all_candidates, key = lambda r: r.sum_log_prob, reverse = True)\n",
    "            sequences = ordered[:k]\n",
    "            \n",
    "            t += 1\n",
    "        final_record = sequences[0]\n",
    "        \n",
    "        predict_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in final_record.predict_word_index_list[1:-1]]\n",
    "        right_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in right_whole_sentence_word_index]\n",
    "\n",
    "        predict_whole_sentence = ' '.join(predict_whole_sentence_word_index)\n",
    "        right_whole_sentence = ' '.join(right_whole_sentence_word_index)\n",
    "\n",
    "        predict_file.write(predict_whole_sentence.strip() + '\\n')\n",
    "        target_file.write(right_whole_sentence.strip() + '\\n')\n",
    "\n",
    "\n",
    "    predict_file.close()\n",
    "    target_file.close()\n",
    "\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file_name,target_file_name),shell=True,stdout=subprocess.PIPE)\n",
    "    result = str(result)\n",
    "    print(result)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    return get_blue_score(result)\n",
    "\n",
    "\n",
    "def get_blue_score(s):\n",
    "    a = re.search(r'13a\\+version\\.1\\.2\\.12 = ([0-9.]+)',s)\n",
    "    return float(a.group(1))\n",
    "\n",
    "\n",
    "\n",
    "def parameters_list_change_grad(encoder, decoder):\n",
    "    para_list = []\n",
    "    for name, data in list(encoder.named_parameters()):\n",
    "        if 'src_embed' in name:\n",
    "            data.requires_grad = False\n",
    "        else:\n",
    "            para_list.append(data)\n",
    "            \n",
    "    for name, data in list(decoder.named_parameters()):\n",
    "        if 'embedding' in name:\n",
    "            data.requires_grad = False\n",
    "        else:\n",
    "            para_list.append(data)\n",
    "    return para_list        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder,decoder = make_model(src_vocab=len(TEXT_vi.vocab.stoi), tgt_vocab=len(TEXT_en.vocab.stoi), N=2, \n",
    "               d_model=512, d_k=64, dropout=0.1)\n",
    "\n",
    "encoder = encoder.cuda(0)\n",
    "decoder = decoder.cuda(0)\n",
    "\n",
    "early_stop = 3\n",
    "best_blue_score = -1\n",
    "best_index = -1\n",
    "\n",
    "save_model_dir_name = '../save_model/vi_to_en_'\n",
    "teacher_forcing_ratio = 0.9\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': encoder.parameters(), 'lr': 0.001},\n",
    "                              {'params': decoder.parameters(), 'lr': 0.001}])\n",
    "\n",
    "\n",
    "for index_unique in range(100):\n",
    "    train(encoder, decoder, optimizer, train_vi_en_iter, teacher_forcing_ratio)\n",
    "    blue_score = test(encoder, decoder, validation_vi_en_iter)\n",
    "    print('epoch: ',index_unique, ' the blue score on validation dataset is : ', blue_score)\n",
    "    sys.stdout.flush()\n",
    "    if best_blue_score < blue_score:\n",
    "        \n",
    "        best_index = index_unique\n",
    "        best_blue_score = blue_score\n",
    "        torch.save(encoder, save_model_dir_name+'cnn_encode')\n",
    "        torch.save(decoder, save_model_dir_name+'rnn_decoder')\n",
    "        \n",
    "    if index_unique - best_index >= early_stop:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('--------------------------------------')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "encoder = torch.load(save_model_dir_name+'cnn_encode')\n",
    "decoder = torch.load(save_model_dir_name+'rnn_decoder')\n",
    "        \n",
    "        \n",
    "\n",
    "para_list = parameters_list_change_grad(encoder, decoder)     \n",
    "optimizer = torch.optim.Adam(para_list, lr = 0.001)  \n",
    "save_model_dir_name = '../save_model/refined_vi_to_en_'\n",
    "\n",
    "early_stop = 3\n",
    "best_blue_score = -1\n",
    "best_index = -1\n",
    "\n",
    "for index_unique in range(100):\n",
    "    train(encoder, decoder, optimizer, train_vi_en_iter, teacher_forcing_ratio)\n",
    "    blue_score = test(encoder, decoder, validation_vi_en_iter)\n",
    "    print('epoch: ',index_unique, ' the blue score on validation dataset is : ', blue_score)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if best_blue_score < blue_score:\n",
    "        \n",
    "        best_index = index_unique\n",
    "        best_blue_score = blue_score\n",
    "        torch.save(encoder, save_model_dir_name+'cnn_encode_'+str(index_unique))\n",
    "        torch.save(decoder, save_model_dir_name+'rnn_decoder_'+str(index_unique))\n",
    "    if index_unique - best_index >= early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Bi_Multi_Layer_LSTM_Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type LSTM_Decoder_With_Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "class Bi_Multi_Layer_LSTM_Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size = 512, hidden_size = 512, dropout = 0.15):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 1\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.dropout_layer = torch.nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.bidirectional = True\n",
    "        self.embedding_layer = torch.nn.Embedding(num_vocab, self.input_size)\n",
    "        self.lstm = torch.nn.LSTM(input_size= self.input_size, hidden_size = self.hidden_size, batch_first = False,\n",
    "                                 bidirectional = self.bidirectional, num_layers = self.num_layers)\n",
    "        \n",
    "        h_0 = torch.zeros(1, self.hidden_size)\n",
    "        torch.nn.init.normal_(h_0, mean=0, std=0.0001)\n",
    "        self.h_0 = torch.nn.Parameter(h_0,requires_grad=True)\n",
    "        \n",
    "        \n",
    "        c_0 = torch.zeros(1, self.hidden_size)\n",
    "        torch.nn.init.normal_(c_0, mean=0, std=0.0001)\n",
    "        self.c_0 = torch.nn.Parameter(c_0,requires_grad=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            h_1 = torch.zeros(1, self.hidden_size)\n",
    "            torch.nn.init.normal_(h_1, mean=0, std=0.0001)\n",
    "            self.h_1 = torch.nn.Parameter(h_1,requires_grad=True)\n",
    "            \n",
    "            \n",
    "            c_1 = torch.zeros(1, self.hidden_size)\n",
    "            torch.nn.init.normal_(c_1, mean=0, std=0.0001)\n",
    "            self.c_1 = torch.nn.Parameter(c_1,requires_grad=True)\n",
    "            \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X_data,X_len = X\n",
    "        #X_data: source_len, 1, input_size    X_len:1,1\n",
    "        \n",
    "        X_data = self.embedding_layer(X_data)\n",
    "        \n",
    "        h_0 = torch.cat([self.h_0]*len(X_len), dim=0).unsqueeze(1)\n",
    "        c_0 = torch.cat([self.c_0]*len(X_len), dim=0).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            h_1 = torch.cat([self.h_1]*len(X_len), dim=0).unsqueeze(1)\n",
    "            c_1 = torch.cat([self.c_1]*len(X_len), dim=0).unsqueeze(1)\n",
    "            \n",
    "            h = torch.cat([h_0,h_1], dim=0)\n",
    "            c = torch.cat([c_0,c_1], dim=0)   \n",
    "\n",
    "        output, (h_n, c_n) = self.lstm(X_data, (h, c))\n",
    "        #output: source_len, 1, 2*hidden_size\n",
    "        h_n = h_n.view(self.num_layers, 2, len(X_len), self.hidden_size)\n",
    "        c_n = c_n.view(self.num_layers, 2, len(X_len), self.hidden_size)\n",
    "        \n",
    "        \n",
    "        return output, h_n, c_n\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        \n",
    "        for name, matrix in self.lstm.named_parameters():\n",
    "            if 'weight_hh_' in name:\n",
    "                for i in range(0, matrix.size(0), self.hidden_size):\n",
    "                    torch.nn.init.orthogonal_(matrix[i:i+self.hidden_size], gain=0.01)\n",
    "            elif 'bias_' in name:\n",
    "                l = len(matrix)\n",
    "                matrix[l // 4: l //2].data.fill_(1.0)\n",
    "                \n",
    "                \n",
    "class LSTM_Decoder_With_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size = 512, hidden_size = 512, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 1\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = torch.nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(self.num_vocab, self.input_size)\n",
    "        self.lstm = torch.nn.LSTM(hidden_size= self.hidden_size, input_size= self.input_size + 2 * self.hidden_size, \n",
    "                                  num_layers= self.num_layers)\n",
    "        \n",
    "        self.calcu_weight_1  = torch.nn.Linear(3*self.hidden_size, hidden_size)\n",
    "        self.calcu_weight_2  = torch.nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        self.init_weight_1 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.init_weight_2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.linear_vob = torch.nn.Linear(self.hidden_size, self.num_vocab)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_word_index, hidden_vector, cell_vector, encoder_memory, is_init = False):\n",
    "        #input_word_index: [num]\n",
    "        #hidden_vector: 1, 1, hidden_size\n",
    "        #cell_vector: 1, 1, hidden_size\n",
    "        #encoder_memory: source_sen_len , 2 * hidden_size\n",
    "        \n",
    "        if hidden_vector.shape[0] != self.num_layers or hidden_vector.shape[2] != self.hidden_size:\n",
    "            raise ValueError('The size of hidden_vector is not correct, expect '+str((self.num_layers, self.hidden_size))\\\n",
    "                            + ', actually get ' + str(hidden_vector.shape))\n",
    "        \n",
    "        if is_init:\n",
    "            hidden_vector = torch.tanh(self.init_weight_1(hidden_vector))\n",
    "            cell_vector = torch.tanh(self.init_weight_2(cell_vector))\n",
    "            \n",
    "        \n",
    "        \n",
    "        n_hidden_vector = torch.stack([hidden_vector.squeeze()]*encoder_memory.shape[0],dim=0)\n",
    "        com_n_h_memory = torch.cat([n_hidden_vector, encoder_memory], dim =1)\n",
    "        com_n_h_temp = torch.tanh(self.calcu_weight_1(com_n_h_memory))\n",
    "        \n",
    "        \n",
    "        weight_vector = self.calcu_weight_2(com_n_h_temp)\n",
    "        weight_vector =  torch.nn.functional.softmax(weight_vector, dim=0)\n",
    "        #weight_vector: source_sen_len * 1\n",
    "        \n",
    "        \n",
    "        convect_vector = torch.mm(weight_vector.transpose(1,0), encoder_memory)\n",
    "        #convect_vector: 1 , 2 * hidden_size\n",
    "        \n",
    "        \n",
    "        input_vector = self.embedding_layer(input_word_index).view(1,1,-1)\n",
    "        input_vector = self.dropout_layer(input_vector)\n",
    "        \n",
    "        \n",
    "        input_vector = torch.cat([convect_vector.unsqueeze(0), input_vector], dim=2)\n",
    "        \n",
    "        \n",
    "        output, (h_t, c_t) = self.lstm(input_vector,(hidden_vector, cell_vector))\n",
    "        output = output.view(1, self.hidden_size)\n",
    "        \n",
    "        \n",
    "        prob = self.linear_vob(output)\n",
    "        #prob 1, vob_size\n",
    "        \n",
    "        prob = torch.nn.functional.log_softmax(prob, dim=1)\n",
    "        \n",
    "        \n",
    "        return prob, h_t, c_t\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        \n",
    "        for name, matrix in self.lstm.named_parameters():\n",
    "            if 'weight_hh_' in name:\n",
    "                for i in range(0, matrix.size(0), self.hidden_size):\n",
    "                    torch.nn.init.orthogonal_(matrix[i:i+self.hidden_size], gain=0.01)\n",
    "            elif 'bias_' in name:\n",
    "                l = len(matrix)\n",
    "                matrix[l // 4: l //2].data.fill_(1.0)\n",
    "    \n",
    "encoder = torch.load('../../save_model/vi_to_en_encode_9')\n",
    "decoder = torch.load('../../save_model/vi_to_en_decoder_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_1 = torch.load('../../save_model/vi_to_en_encode_5')\n",
    "decoder_1 = torch.load('../../save_model/vi_to_en_decoder_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_list = [encoder, encoder_1]\n",
    "decoder_list = [decoder, decoder_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bean_Search_Status_Record:\n",
    "    \n",
    "    def __init__(self, h_t_list, c_t_list, predict_word_index_list, sum_log_prob):\n",
    "        self.h_t_list = h_t_list\n",
    "        self.c_t_list = c_t_list\n",
    "        self.predict_word_index_list = predict_word_index_list\n",
    "        self.sum_log_prob = sum_log_prob\n",
    "        self.avg_log_prob = 0\n",
    "\n",
    "def beam_search_ensembel_test(encoder_list, decoder_list, data_iter, k=10):\n",
    "    \n",
    "    assert len(encoder_list) == len(decoder_list), 'the num of encoders should be equal to the num of decoders'\n",
    "    _ = [model.eval() for model in encoder_list]\n",
    "    _ = [model.eval() for model in decoder_list]\n",
    "\n",
    "    path_name = '../eval/'+str(time.time()).replace('.','_')+'/'\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "    predict_file_name = path_name + 'predict.txt'\n",
    "    target_file_name = path_name + 'target_file_name.txt'\n",
    "\n",
    "    predict_file = open(predict_file_name, 'w')\n",
    "    target_file = open(target_file_name, 'w')\n",
    "\n",
    "\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        h_t_list = []\n",
    "        c_t_list = []\n",
    "        output_list = []\n",
    "        \n",
    "        for encoder in encoder_list:\n",
    "            all_output, h_n, c_n = encoder(source)\n",
    "            output = all_output[:,0]\n",
    "            h_t = h_n[:,1,:]\n",
    "            c_t = c_n[:,1,:]\n",
    "            h_t_list.append(h_t)\n",
    "            c_t_list.append(c_t)\n",
    "            output_list.append(output)\n",
    "            \n",
    "\n",
    "        target_word = TEXT_en.vocab.stoi['<sos>']\n",
    "\n",
    "\n",
    "        is_init = False\n",
    "\n",
    "\n",
    "        right_whole_sentence_word_index = target_data[1: target_len[0].item()-1,0]\n",
    "        right_whole_sentence_word_index = list(right_whole_sentence_word_index.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        sequences = [Bean_Search_Status_Record(h_t_list, c_t_list, predict_word_index_list = [target_word], \n",
    "                                               sum_log_prob = 0.0)]\n",
    "        \n",
    "        t = 0\n",
    "        while (t < 60):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                record = sequences[i]\n",
    "                \n",
    "                h_t_list = record.h_t_list\n",
    "                c_t_list = record.c_t_list\n",
    "                predict_word_index_list = record.predict_word_index_list\n",
    "                sum_log_prob = record.sum_log_prob\n",
    "                target_word = predict_word_index_list[-1]\n",
    "                \n",
    "                temp_h_t_list = []\n",
    "                temp_c_t_list = []\n",
    "                temp_prob = None\n",
    "                \n",
    "                if TEXT_en.vocab.stoi['<eos>'] != target_word:\n",
    "                    for num_model in range(len(encoder_list)):\n",
    "                        \n",
    "                        decoder = decoder_list[num_model]\n",
    "                        h_t = h_t_list[num_model]\n",
    "                        c_t = c_t_list[num_model]\n",
    "                        output = output_list[num_model]\n",
    "                \n",
    "                        prob, h_t, c_t = decoder(torch.tensor([target_word]).cuda(0), h_t, c_t, output, is_init)\n",
    "                    \n",
    "                        temp_h_t_list.append(h_t)\n",
    "                        temp_c_t_list.append(c_t)\n",
    "                        \n",
    "                        if temp_prob is None:\n",
    "                            temp_prob = prob\n",
    "                        else:\n",
    "                            temp_prob = torch.cat([temp_prob, prob], dim=0)\n",
    "                            \n",
    "                    \n",
    "                            \n",
    "                            \n",
    "                    prob = temp_prob.mean(dim=0, keepdim=True)\n",
    "                    k_prob_value_list, k_word_index_list = prob.topk(k,dim=1)\n",
    "                    k_prob_value_list = k_prob_value_list.cpu().detach().squeeze().numpy()\n",
    "                    k_word_index_list = k_word_index_list.cpu().squeeze().numpy()\n",
    "\n",
    "\n",
    "                    for prob_value, word_index in zip(k_prob_value_list, k_word_index_list):\n",
    "                        prob_value = float(prob_value)\n",
    "                        word_index = int(word_index)\n",
    "                        new_record = Bean_Search_Status_Record(temp_h_t_list, temp_c_t_list, predict_word_index_list+[word_index], sum_log_prob+prob_value)\n",
    "                        new_record.avg_log_prob = new_record.sum_log_prob/((4+len(new_record.predict_word_index_list))**0.6/(6)**0.6)\n",
    "                        all_candidates.append(new_record)\n",
    "                else:\n",
    "                    all_candidates.append(record)\n",
    "            is_init = False\n",
    "                        \n",
    "            ordered = sorted(all_candidates, key = lambda r: r.avg_log_prob, reverse = True)\n",
    "            sequences = ordered[:k]\n",
    "            \n",
    "            t += 1\n",
    "        final_record = sequences[0]\n",
    "        \n",
    "        \n",
    "        predict_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in final_record.predict_word_index_list[1:-1]]\n",
    "        right_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in right_whole_sentence_word_index]\n",
    "\n",
    "        predict_whole_sentence = ' '.join(predict_whole_sentence_word_index)\n",
    "        right_whole_sentence = ' '.join(right_whole_sentence_word_index)\n",
    "\n",
    "        predict_file.write(predict_whole_sentence.strip() + '\\n')\n",
    "        target_file.write(right_whole_sentence.strip() + '\\n')\n",
    "\n",
    "\n",
    "    predict_file.close()\n",
    "    target_file.close()\n",
    "\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file_name,target_file_name),shell=True,stdout=subprocess.PIPE)\n",
    "    result = str(result)\n",
    "    print(result)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    return get_blue_score(result)\n",
    "\n",
    "\n",
    "    \n",
    "def get_blue_score(s):\n",
    "    a = re.search(r'13a\\+version\\.1\\.2\\.12 = ([0-9.]+)',s)\n",
    "    return float(a.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:137: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-32d296dc1b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search_ensembel_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_vi_en_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e5eae1e940eb>\u001b[0m in \u001b[0;36mbeam_search_ensembel_test\u001b[0;34m(encoder_list, decoder_list, data_iter, k)\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                         \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         \u001b[0mtemp_h_t_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-42eeef50fc38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_word_index, hidden_vector, cell_vector, encoder_memory, is_init)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "r = beam_search_ensembel_test([encoder, encoder_1], [decoder, decoder_1], validation_vi_en_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:137: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='cat ../eval/1543272655_7849529/predict.txt | sacrebleu ../eval/1543272655_7849529/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 18.6 53.1/25.7/14.3/8.6 (BP = 0.918 ratio = 0.921 hyp_len = 26043 ref_len = 28283)\\n')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "r = test([encoder, encoder_1], [decoder, decoder_1], validation_vi_en_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 7,\n",
       " 801,\n",
       " 4,\n",
       " 10,\n",
       " 116,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 25,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 15,\n",
       " 46,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 5,\n",
       " 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.predict_word_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:137: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='cat ../eval/1543265601_7490401/predict.txt | sacrebleu ../eval/1543265601_7490401/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 18.3 51.6/24.8/14.0/8.4 (BP = 0.929 ratio = 0.931 hyp_len = 27954 ref_len = 30020)\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_vi_en = torchtext.data.TabularDataset('../data/processed_data/test_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "  \n",
    "test(encoder, decoder, validation_vi_en_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bean_Search_Status_Record:\n",
    "    \n",
    "    def __init__(self, h_t, c_t, predict_word_index_list, sum_log_prob):\n",
    "        self.h_t = h_t\n",
    "        self.c_t = c_t\n",
    "        self.predict_word_index_list = predict_word_index_list\n",
    "        self.sum_log_prob = sum_log_prob\n",
    "        self.avg_log_prob = 0\n",
    "        \n",
    "    \n",
    "\n",
    "def test(encoder, decoder, data_iter, k=5):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    path_name = '../eval/'+str(time.time()).replace('.','_')+'/'\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "    predict_file_name = path_name + 'predict.txt'\n",
    "    target_file_name = path_name + 'target_file_name.txt'\n",
    "\n",
    "    predict_file = open(predict_file_name, 'w')\n",
    "    target_file = open(target_file_name, 'w')\n",
    "\n",
    "\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output, h_n, c_n = encoder(source)\n",
    "        output = all_output[:,0]\n",
    "\n",
    "        target_word = TEXT_en.vocab.stoi['<sos>']\n",
    "\n",
    "        h_t = h_n[:,1,:]\n",
    "        c_t = c_n[:,1,:]\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "\n",
    "        right_whole_sentence_word_index = target_data[1: target_len[0].item()-1,0]\n",
    "        right_whole_sentence_word_index = list(right_whole_sentence_word_index.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        sequences = [Bean_Search_Status_Record(h_t, c_t, predict_word_index_list = [target_word], \n",
    "                                               sum_log_prob = 0.0)]\n",
    "        \n",
    "        t = 0\n",
    "        while (t < 100):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                record = sequences[i]\n",
    "                h_t = record.h_t\n",
    "                c_t = record.c_t\n",
    "                predict_word_index_list = record.predict_word_index_list\n",
    "                sum_log_prob = record.sum_log_prob\n",
    "                target_word = predict_word_index_list[-1]\n",
    "                \n",
    "                if TEXT_en.vocab.stoi['<eos>'] != target_word:\n",
    "                \n",
    "                    prob, h_t, c_t = decoder(torch.tensor([target_word]).cuda(0), h_t, c_t, output, is_init)\n",
    "\n",
    "                    k_prob_value_list, k_word_index_list = prob.topk(k,dim=1)\n",
    "                    k_prob_value_list = k_prob_value_list.cpu().detach().squeeze().numpy()\n",
    "                    k_word_index_list = k_word_index_list.cpu().squeeze().numpy()\n",
    "                    \n",
    "                    for prob_value, word_index in zip(k_prob_value_list, k_word_index_list):\n",
    "                        prob_value = float(prob_value)\n",
    "                        word_index = int(word_index)\n",
    "                        new_record = Bean_Search_Status_Record(h_t, c_t, predict_word_index_list+[word_index], sum_log_prob+prob_value)\n",
    "                        new_record.avg_log_prob = new_record.sum_log_prob/(len(new_record.predict_word_index_list) - 1)\n",
    "                        all_candidates.append(new_record)\n",
    "                else:\n",
    "                    all_candidates.append(record)\n",
    "            is_init = False\n",
    "                        \n",
    "            ordered = sorted(all_candidates, key = lambda r: r.sum_log_prob, reverse = True)\n",
    "            sequences = ordered[:k]\n",
    "            \n",
    "            t += 1\n",
    "        final_record = sequences[0]\n",
    "        \n",
    "        predict_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in final_record.predict_word_index_list[1:-1]]\n",
    "        right_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in right_whole_sentence_word_index]\n",
    "\n",
    "        predict_whole_sentence = ' '.join(predict_whole_sentence_word_index)\n",
    "        right_whole_sentence = ' '.join(right_whole_sentence_word_index)\n",
    "\n",
    "        predict_file.write(predict_whole_sentence.strip() + '\\n')\n",
    "        target_file.write(right_whole_sentence.strip() + '\\n')\n",
    "\n",
    "\n",
    "    predict_file.close()\n",
    "    target_file.close()\n",
    "\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file_name,target_file_name),shell=True,stdout=subprocess.PIPE)\n",
    "    result = str(result)\n",
    "    print(result)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    return get_blue_score(result)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchtext\n",
    "import pickle\n",
    "import csv\n",
    "import unicodedata\n",
    "import re, random, time, string, subprocess\n",
    "import os, sys\n",
    "\n",
    "\n",
    "TEXT_vi = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = False, tokenize= lambda t:t.split(),\n",
    "                                        include_lengths=True)\n",
    "TEXT_en = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = False, tokenize= lambda t:t.split(),\n",
    "                              lower=True, init_token='<sos>', eos_token='<eos>',include_lengths=True)\n",
    "train_vi_en = torchtext.data.TabularDataset('../data/processed_data/train_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "validation_vi_en = torchtext.data.TabularDataset('../data/processed_data/dev_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "\n",
    "\n",
    "TEXT_vi.build_vocab(train_vi_en)\n",
    "TEXT_en.build_vocab(train_vi_en)\n",
    "\n",
    "\n",
    "train_vi_en_iter = torchtext.data.BucketIterator(train_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "\n",
    "\n",
    "class Bi_Multi_Layer_LSTM_Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size = 512, hidden_size = 512, dropout = 0.15):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 1\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.dropout_layer = torch.nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.bidirectional = True\n",
    "        self.embedding_layer = torch.nn.Embedding(num_vocab, self.input_size)\n",
    "        self.lstm = torch.nn.LSTM(input_size= self.input_size, hidden_size = self.hidden_size, batch_first = False,\n",
    "                                 bidirectional = self.bidirectional, num_layers = self.num_layers)\n",
    "        \n",
    "        h_0 = torch.zeros(1, self.hidden_size)\n",
    "        torch.nn.init.normal_(h_0, mean=0, std=0.0001)\n",
    "        self.h_0 = torch.nn.Parameter(h_0,requires_grad=True)\n",
    "        \n",
    "        \n",
    "        c_0 = torch.zeros(1, self.hidden_size)\n",
    "        torch.nn.init.normal_(c_0, mean=0, std=0.0001)\n",
    "        self.c_0 = torch.nn.Parameter(c_0,requires_grad=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            h_1 = torch.zeros(1, self.hidden_size)\n",
    "            torch.nn.init.normal_(h_1, mean=0, std=0.0001)\n",
    "            self.h_1 = torch.nn.Parameter(h_1,requires_grad=True)\n",
    "            \n",
    "            \n",
    "            c_1 = torch.zeros(1, self.hidden_size)\n",
    "            torch.nn.init.normal_(c_1, mean=0, std=0.0001)\n",
    "            self.c_1 = torch.nn.Parameter(c_1,requires_grad=True)\n",
    "            \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X_data,X_len = X\n",
    "        #X_data: source_len, 1, input_size    X_len:1,1\n",
    "        \n",
    "        X_data = self.embedding_layer(X_data)\n",
    "        \n",
    "        h_0 = torch.cat([self.h_0]*len(X_len), dim=0).unsqueeze(1)\n",
    "        c_0 = torch.cat([self.c_0]*len(X_len), dim=0).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            h_1 = torch.cat([self.h_1]*len(X_len), dim=0).unsqueeze(1)\n",
    "            c_1 = torch.cat([self.c_1]*len(X_len), dim=0).unsqueeze(1)\n",
    "            \n",
    "            h = torch.cat([h_0,h_1], dim=0)\n",
    "            c = torch.cat([c_0,c_1], dim=0)   \n",
    "\n",
    "        output, (h_n, c_n) = self.lstm(X_data, (h, c))\n",
    "        #output: source_len, 1, 2*hidden_size\n",
    "        h_n = h_n.view(self.num_layers, 2, len(X_len), self.hidden_size)\n",
    "        c_n = c_n.view(self.num_layers, 2, len(X_len), self.hidden_size)\n",
    "        \n",
    "        \n",
    "        return output, h_n, c_n\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        \n",
    "        for name, matrix in self.lstm.named_parameters():\n",
    "            if 'weight_hh_' in name:\n",
    "                for i in range(0, matrix.size(0), self.hidden_size):\n",
    "                    torch.nn.init.orthogonal_(matrix[i:i+self.hidden_size], gain=0.01)\n",
    "            elif 'bias_' in name:\n",
    "                l = len(matrix)\n",
    "                matrix[l // 4: l //2].data.fill_(1.0)\n",
    "                \n",
    "                \n",
    "class LSTM_Decoder_With_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size = 512, hidden_size = 512, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 1\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = torch.nn.Dropout(self.dropout)\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(self.num_vocab, self.input_size)\n",
    "        self.lstm = torch.nn.LSTM(hidden_size= self.hidden_size, input_size= self.input_size + 2 * self.hidden_size, \n",
    "                                  num_layers= self.num_layers)\n",
    "        \n",
    "        self.calcu_weight_1  = torch.nn.Linear(3*self.hidden_size, hidden_size)\n",
    "        self.calcu_weight_2  = torch.nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        self.init_weight_1 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.init_weight_2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.linear_vob = torch.nn.Linear(self.hidden_size, self.num_vocab)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_word_index, hidden_vector, cell_vector, encoder_memory, is_init = False):\n",
    "        #input_word_index: [num]\n",
    "        #hidden_vector: 1, 1, hidden_size\n",
    "        #cell_vector: 1, 1, hidden_size\n",
    "        #encoder_memory: source_sen_len , 2 * hidden_size\n",
    "        \n",
    "        if hidden_vector.shape[0] != self.num_layers or hidden_vector.shape[2] != self.hidden_size:\n",
    "            raise ValueError('The size of hidden_vector is not correct, expect '+str((self.num_layers, self.hidden_size))\\\n",
    "                            + ', actually get ' + str(hidden_vector.shape))\n",
    "        \n",
    "        if is_init:\n",
    "            hidden_vector = torch.tanh(self.init_weight_1(hidden_vector))\n",
    "            cell_vector = torch.tanh(self.init_weight_2(cell_vector))\n",
    "            \n",
    "        \n",
    "        \n",
    "        n_hidden_vector = torch.stack([hidden_vector.squeeze()]*encoder_memory.shape[0],dim=0)\n",
    "        com_n_h_memory = torch.cat([n_hidden_vector, encoder_memory], dim =1)\n",
    "        com_n_h_temp = torch.tanh(self.calcu_weight_1(com_n_h_memory))\n",
    "        \n",
    "        \n",
    "        weight_vector = self.calcu_weight_2(com_n_h_temp)\n",
    "        weight_vector =  torch.nn.functional.softmax(weight_vector, dim=0)\n",
    "        #weight_vector: source_sen_len * 1\n",
    "        \n",
    "        \n",
    "        convect_vector = torch.mm(weight_vector.transpose(1,0), encoder_memory)\n",
    "        #convect_vector: 1 , 2 * hidden_size\n",
    "        \n",
    "        \n",
    "        input_vector = self.embedding_layer(input_word_index).view(1,1,-1)\n",
    "        input_vector = self.dropout_layer(input_vector)\n",
    "        \n",
    "        \n",
    "        input_vector = torch.cat([convect_vector.unsqueeze(0), input_vector], dim=2)\n",
    "        \n",
    "        \n",
    "        output, (h_t, c_t) = self.lstm(input_vector,(hidden_vector, cell_vector))\n",
    "        output = output.view(1, self.hidden_size)\n",
    "        \n",
    "        \n",
    "        prob = self.linear_vob(output)\n",
    "        #prob 1, vob_size\n",
    "        \n",
    "        prob = torch.nn.functional.log_softmax(prob, dim=1)\n",
    "        \n",
    "        \n",
    "        return prob, h_t, c_t\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        \n",
    "        for name, matrix in self.lstm.named_parameters():\n",
    "            if 'weight_hh_' in name:\n",
    "                for i in range(0, matrix.size(0), self.hidden_size):\n",
    "                    torch.nn.init.orthogonal_(matrix[i:i+self.hidden_size], gain=0.01)\n",
    "            elif 'bias_' in name:\n",
    "                l = len(matrix)\n",
    "                matrix[l // 4: l //2].data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, data_iter, k=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    path_name = '../eval/'+str(time.time()).replace('.','_')+'/'\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "    predict_file_name = path_name + 'predict.txt'\n",
    "    target_file_name = path_name + 'target_file_name.txt'\n",
    "\n",
    "    predict_file = open(predict_file_name, 'w')\n",
    "    target_file = open(target_file_name, 'w')\n",
    "\n",
    "\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output, h_n, c_n = encoder(source)\n",
    "        output = all_output[:,0]\n",
    "\n",
    "        target_word = TEXT_en.vocab.stoi['<sos>']\n",
    "\n",
    "        h_t = h_n[:,1,:]\n",
    "        c_t = c_n[:,1,:]\n",
    "\n",
    "        is_init = False\n",
    "\n",
    "\n",
    "        right_whole_sentence_word_index = target_data[1: target_len[0].item()-1,0]\n",
    "        right_whole_sentence_word_index = list(right_whole_sentence_word_index.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        sequences = [Bean_Search_Status_Record(h_t, c_t, predict_word_index_list = [target_word], \n",
    "                                               sum_log_prob = 0.0)]\n",
    "        \n",
    "        t = 0\n",
    "        while (t < 100):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                record = sequences[i]\n",
    "                h_t = record.h_t\n",
    "                c_t = record.c_t\n",
    "                predict_word_index_list = record.predict_word_index_list\n",
    "                sum_log_prob = record.sum_log_prob\n",
    "                target_word = predict_word_index_list[-1]\n",
    "                \n",
    "                if TEXT_en.vocab.stoi['<eos>'] != target_word:\n",
    "                \n",
    "                    prob, h_t, c_t = decoder(torch.tensor([target_word]).cuda(0), h_t, c_t, output, is_init)\n",
    "\n",
    "                    k_prob_value_list, k_word_index_list = prob.topk(k,dim=1)\n",
    "                    k_prob_value_list = k_prob_value_list.cpu().detach().squeeze().numpy()\n",
    "                    k_word_index_list = k_word_index_list.cpu().squeeze().numpy()\n",
    "                    \n",
    "                    \n",
    "                    for prob_value, word_index in zip(k_prob_value_list, k_word_index_list):\n",
    "                        prob_value = float(prob_value)\n",
    "                        word_index = int(word_index)\n",
    "                        new_record = Bean_Search_Status_Record(h_t, c_t, predict_word_index_list+[word_index], sum_log_prob+prob_value)\n",
    "                        new_record.avg_log_prob = new_record.sum_log_prob/(len(new_record.predict_word_index_list) - 1)\n",
    "                        all_candidates.append(new_record)\n",
    "                else:\n",
    "                    all_candidates.append(record)\n",
    "            is_init = False\n",
    "                        \n",
    "            ordered = sorted(all_candidates, key = lambda r: r.sum_log_prob, reverse = True)\n",
    "            sequences = ordered[:k]\n",
    "            \n",
    "            t += 1\n",
    "        final_record = sequences[0]\n",
    "        \n",
    "        predict_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in final_record.predict_word_index_list[1:-1]]\n",
    "        right_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in right_whole_sentence_word_index]\n",
    "\n",
    "        predict_whole_sentence = ' '.join(predict_whole_sentence_word_index)\n",
    "        right_whole_sentence = ' '.join(right_whole_sentence_word_index)\n",
    "\n",
    "        predict_file.write(predict_whole_sentence.strip() + '\\n')\n",
    "        target_file.write(right_whole_sentence.strip() + '\\n')\n",
    "\n",
    "\n",
    "    predict_file.close()\n",
    "    target_file.close()\n",
    "\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file_name,target_file_name),shell=True,stdout=subprocess.PIPE)\n",
    "    result = str(result)\n",
    "    print(result)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    return get_blue_score(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Bi_Multi_Layer_LSTM_Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type LSTM_Decoder_With_Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "encoder = torch.load('../save_model/vi_to_en_encode_6')\n",
    "decoder = torch.load('../save_model/vi_to_en_decoder_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:166: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='cat ../eval/1543033030_7136176/predict.txt | sacrebleu ../eval/1543033030_7136176/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 16.5 50.3/22.9/12.1/6.8 (BP = 0.940 ratio = 0.941 hyp_len = 26626 ref_len = 28283)\\n')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_blue_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e41414b942d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                              fields=[('source',TEXT_vi),('target',TEXT_en)])\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_vi_en_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-b4b440605d5d>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(encoder, decoder, data_iter, k)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_blue_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_blue_score' is not defined"
     ]
    }
   ],
   "source": [
    "class Bean_Search_Status_Record:\n",
    "    \n",
    "    def __init__(self, h_t, c_t, predict_word_index_list, sum_log_prob):\n",
    "        self.h_t = h_t\n",
    "        self.c_t = c_t\n",
    "        self.predict_word_index_list = predict_word_index_list\n",
    "        self.sum_log_prob = sum_log_prob\n",
    "        self.avg_log_prob = 0\n",
    "        \n",
    "validation_vi_en = torchtext.data.TabularDataset('../data/processed_data/test_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "  \n",
    "test(encoder, decoder, validation_vi_en_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='cat ../eval/1542906497_4235842/predict.txt | sacrebleu ../eval/1542906497_4235842/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 0.1 2.0/0.1/0.0/0.0 (BP = 1.000 ratio = 2.962 hyp_len = 83763 ref_len = 28283)\\n')\n",
      "epoch:  0  the blue score on validation dataset is :  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Bi_Multi_Layer_GRU_Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type GRU_Decoder_With_Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args='cat ../eval/1542907219_6010282/predict.txt | sacrebleu ../eval/1542907219_6010282/target_file_name.txt', returncode=0, stdout=b'BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.2.12 = 0.0 23.6/0.0/0.0/0.0 (BP = 0.038 ratio = 0.235 hyp_len = 6635 ref_len = 28283)\\n')\n",
      "epoch:  1  the blue score on validation dataset is :  0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "import pickle\n",
    "import csv\n",
    "import unicodedata\n",
    "import re, random, time, string, subprocess, sys\n",
    "import os, re\n",
    "\n",
    "\n",
    "TEXT_vi = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = False, tokenize= lambda t:t.split(),\n",
    "                                        include_lengths=True)\n",
    "TEXT_en = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = False, tokenize= lambda t:t.split(),\n",
    "                              lower=True, init_token='<sos>', eos_token='<eos>',include_lengths=True)\n",
    "train_vi_en = torchtext.data.TabularDataset('../data/processed_data/train_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "validation_vi_en = torchtext.data.TabularDataset('../data/processed_data/dev_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "\n",
    "\n",
    "TEXT_vi.build_vocab(train_vi_en)\n",
    "TEXT_en.build_vocab(train_vi_en)\n",
    "\n",
    "train_vi_en_iter = torchtext.data.BucketIterator(train_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Bi_Multi_Layer_GRU_Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size, hidden_size, num_layers = 2, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = True\n",
    "        self.embedding_layer = torch.nn.Embedding(num_vocab, self.input_size)\n",
    "        self.gru = torch.nn.GRU(input_size= self.input_size, hidden_size = self.hidden_size, batch_first = False,\n",
    "                                 bidirectional = self.bidirectional, dropout = self.dropout, \n",
    "                                  num_layers = self.num_layers)\n",
    "        \n",
    "        h_0 = torch.zeros(1, self.hidden_size)\n",
    "        torch.nn.init.normal_(h_0, mean=0, std=0.001)\n",
    "        self.h_0 = torch.nn.Parameter(h_0,requires_grad=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            h_1 = torch.zeros(1, self.hidden_size)\n",
    "            torch.nn.init.normal_(h_1, mean=0, std=0.001)\n",
    "            self.h_1 = torch.nn.Parameter(h_1,requires_grad=True)\n",
    "            \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X_data,X_len = X\n",
    "        \n",
    "        X_data = self.embedding_layer(X_data)\n",
    "        \n",
    "        h_0 = torch.cat([self.h_0]*len(X_len), dim=0).unsqueeze(0)\n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            h_1 = torch.cat([self.h_1]*len(X_len), dim=0).unsqueeze(0)\n",
    "            \n",
    "            h = torch.cat([h_0,h_1], dim=0)\n",
    "            \n",
    "            \n",
    "            h = torch.cat([h]*self.num_layers, dim=0)\n",
    "           \n",
    "            \n",
    "\n",
    "        output, h_n = self.gru(X_data, h)\n",
    "        #seq_len, batch, num_directions * hidden_size\n",
    "        \n",
    "        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "class GRU_Decoder_With_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 1\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(self.num_vocab, self.input_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size= self.hidden_size, input_size= self.input_size + 2 * self.hidden_size, \n",
    "                                  num_layers= self.num_layers)\n",
    "        \n",
    "        self.calcu_weight_1  = torch.nn.Linear(3*self.hidden_size, hidden_size)\n",
    "        self.calcu_weight_2  = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.init_weight = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.linear_vob = torch.nn.Linear(self.hidden_size, self.num_vocab)\n",
    "        \n",
    "    def forward(self, input_word_index, hidden_vector, encoder_memory, is_init = False):\n",
    "        #input_word_index: [num]\n",
    "        #hidden_vector: 1, 1, hidden_size\n",
    "        #encoder_memory: source_sen_len , 2 * hidden_size\n",
    "        \n",
    "        if hidden_vector.shape[0] != self.num_layers or hidden_vector.shape[2] != self.hidden_size:\n",
    "            raise ValueError('The size of hidden_vector is not correct, expect '+str((self.num_layers, self.hidden_size))\\\n",
    "                            + ', actually get ' + str(hidden_vector.shape))\n",
    "        \n",
    "        if is_init:\n",
    "            hidden_vector = torch.tanh(self.init_weight(hidden_vector))\n",
    "        \n",
    "        \n",
    "        n_hidden_vector = torch.stack([hidden_vector.squeeze()]*encoder_memory.shape[0],dim=0)\n",
    "        com_n_h_memory = torch.cat([n_hidden_vector, encoder_memory], dim =1)\n",
    "        com_n_h_temp = torch.tanh(self.calcu_weight_1(com_n_h_memory))\n",
    "        \n",
    "        \n",
    "        weight_vector = self.calcu_weight_2(com_n_h_temp)\n",
    "        weight_vector =  torch.nn.functional.softmax(weight_vector, dim=0)\n",
    "        #weight_vector: source_sen_len * 1\n",
    "        \n",
    "        \n",
    "        convect_vector = torch.mm(weight_vector.transpose(1,0), encoder_memory)\n",
    "        #convect_vector: 1 , 2 * hidden_size\n",
    "        \n",
    "        \n",
    "        input_vector = self.embedding_layer(input_word_index).view(1,1,-1)\n",
    "        \n",
    "        \n",
    "        input_vector = torch.cat([convect_vector.unsqueeze(0), input_vector], dim=2)\n",
    "        \n",
    "        \n",
    "        output, h_t = self.gru(input_vector,hidden_vector)\n",
    "        output = output.view(1, self.hidden_size)\n",
    "        \n",
    "        \n",
    "        prob = self.linear_vob(output)\n",
    "        #prob 1, vob_size\n",
    "        \n",
    "        prob = torch.nn.functional.log_softmax(prob, dim=1)\n",
    "        \n",
    "        \n",
    "        return prob, h_t\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def train(encoder, decoder, optimizer, data_iter, teacher_forcing_ratio, batch_size = 32):\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    count = 0\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output = encoder(source)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "\n",
    "        output = all_output[:,0]\n",
    "        target_word_list = target_data.squeeze()\n",
    "        target_word = torch.tensor([TEXT_en.vocab.stoi['<sos>']]).cuda(0)\n",
    "\n",
    "        h_t = output[0,int(output.shape[1]/2):]\n",
    "        h_t = h_t.view([1,1,-1])\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "        for word_index in range(1, target_len[0].item()):\n",
    "            prob, h_t = decoder(target_word, h_t, output, is_init)\n",
    "            is_init = False\n",
    "            if use_teacher_forcing:\n",
    "                target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, target_word)\n",
    "            else:\n",
    "                right_target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, right_target_word)\n",
    "                predict_target_word_index = prob.topk(1)[1].item()\n",
    "\n",
    "                if TEXT_en.vocab.stoi['<eos>'] == predict_target_word_index:\n",
    "                    break\n",
    "                else:\n",
    "                    target_word = torch.tensor([predict_target_word_index]).cuda(0)\n",
    "                    \n",
    "        count += 1\n",
    "        if count % batch_size == 0:\n",
    "            \n",
    "            loss = loss/batch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            count = 0\n",
    "            loss = 0\n",
    "        \n",
    "        \n",
    "    if count % batch_size != 0:\n",
    "        loss = loss/count\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        \n",
    "class Bean_Search_Status_Record:\n",
    "    \n",
    "    def __init__(self, h_t, predict_word_index_list, sum_log_prob):\n",
    "        self.h_t = h_t\n",
    "        self.predict_word_index_list = predict_word_index_list\n",
    "        self.sum_log_prob = sum_log_prob\n",
    "        self.avg_log_prob = 0\n",
    "        \n",
    "    \n",
    "\n",
    "def test(encoder, decoder, data_iter, k=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    path_name = '../eval/'+str(time.time()).replace('.','_')+'/'\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "    predict_file_name = path_name + 'predict.txt'\n",
    "    target_file_name = path_name + 'target_file_name.txt'\n",
    "\n",
    "    predict_file = open(predict_file_name, 'w')\n",
    "    target_file = open(target_file_name, 'w')\n",
    "\n",
    "\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output = encoder(source)\n",
    "        output = all_output[:,0]\n",
    "        \n",
    "        target_word = torch.tensor([TEXT_en.vocab.stoi['<sos>']]).cuda(0)\n",
    "\n",
    "        h_t = output[0,int(output.shape[1]/2):]\n",
    "        h_t = h_t.view([1,1,-1])\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "\n",
    "        right_whole_sentence_word_index = target_data[1: target_len[0].item()-1,0]\n",
    "        right_whole_sentence_word_index = list(right_whole_sentence_word_index.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        sequences = [Bean_Search_Status_Record(h_t, predict_word_index_list = [target_word], \n",
    "                                               sum_log_prob = 0.0)]\n",
    "        \n",
    "        t = 0\n",
    "        while (t < 100):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                record = sequences[i]\n",
    "                h_t = record.h_t\n",
    "                predict_word_index_list = record.predict_word_index_list\n",
    "                sum_log_prob = record.sum_log_prob\n",
    "                target_word = predict_word_index_list[-1]\n",
    "                \n",
    "                if TEXT_en.vocab.stoi['<eos>'] != target_word:\n",
    "                \n",
    "                    prob, h_t = decoder(torch.tensor([target_word]).cuda(0), h_t, output, is_init)\n",
    "\n",
    "                    k_prob_value_list, k_word_index_list = prob.topk(k,dim=1)\n",
    "                    k_prob_value_list = k_prob_value_list.cpu().detach().squeeze().numpy()\n",
    "                    k_word_index_list = k_word_index_list.cpu().squeeze().numpy()\n",
    "                    \n",
    "                    \n",
    "                    for prob_value, word_index in zip(k_prob_value_list, k_word_index_list):\n",
    "                        prob_value = float(prob_value)\n",
    "                        word_index = int(word_index)\n",
    "                        new_record = Bean_Search_Status_Record(h_t, predict_word_index_list+[word_index], sum_log_prob+prob_value)\n",
    "                        new_record.avg_log_prob = new_record.sum_log_prob/(len(new_record.predict_word_index_list) - 1)\n",
    "                        all_candidates.append(new_record)\n",
    "                else:\n",
    "                    all_candidates.append(record)\n",
    "            is_init = False\n",
    "                        \n",
    "            ordered = sorted(all_candidates, key = lambda r: r.sum_log_prob, reverse = True)\n",
    "            sequences = ordered[:k]\n",
    "            \n",
    "            t += 1\n",
    "        final_record = sequences[0]\n",
    "        \n",
    "        predict_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in final_record.predict_word_index_list[1:-1]]\n",
    "        right_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in right_whole_sentence_word_index]\n",
    "\n",
    "        predict_whole_sentence = ' '.join(predict_whole_sentence_word_index)\n",
    "        right_whole_sentence = ' '.join(right_whole_sentence_word_index)\n",
    "\n",
    "        predict_file.write(predict_whole_sentence.strip() + '\\n')\n",
    "        target_file.write(right_whole_sentence.strip() + '\\n')\n",
    "\n",
    "\n",
    "    predict_file.close()\n",
    "    target_file.close()\n",
    "\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file_name,target_file_name),shell=True,stdout=subprocess.PIPE)\n",
    "    result = str(result)\n",
    "    print(result)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    return get_blue_score(result)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_blue_score(s):\n",
    "    a = re.search(r'13a\\+version\\.1\\.2\\.12 = ([0-9.]+)',s)\n",
    "    return float(a.group(1))\n",
    "\n",
    "\n",
    "def parameters_list(encoder, decoder):\n",
    "    para_list_1 = []\n",
    "    para_list_2 = []\n",
    "    for name, data in list(encoder.named_parameters()):\n",
    "        if 'embedding' in name:\n",
    "            para_list_1.append(data)\n",
    "        else:\n",
    "            para_list_2.append(data)\n",
    "\n",
    "    for name, data in list(decoder.named_parameters()):\n",
    "        if 'embedding' in name:\n",
    "            para_list_1.append(data)\n",
    "        else:\n",
    "            para_list_2.append(data)\n",
    "    return para_list_1, para_list_2\n",
    "\n",
    "\n",
    "def parameters_list_change_grad(encoder, decoder):\n",
    "    para_list = []\n",
    "    for name, data in list(encoder.named_parameters()):\n",
    "        if 'embedding' in name:\n",
    "            data.requires_grad = False\n",
    "        else:\n",
    "            para_list.append(data)\n",
    "\n",
    "    for name, data in list(decoder.named_parameters()):\n",
    "        if 'embedding' in name:\n",
    "            data.requires_grad = False\n",
    "        else:\n",
    "            para_list.append(data)\n",
    "    return para_list\n",
    "\n",
    "\n",
    "\n",
    "encoder = Bi_Multi_Layer_GRU_Encoder(num_vocab=len(TEXT_vi.vocab.stoi), input_size=300, hidden_size=300, num_layers=2)\n",
    "decoder = GRU_Decoder_With_Attention(num_vocab = len(TEXT_en.vocab.stoi), input_size = 300, hidden_size=300, dropout=0.1)\n",
    "\n",
    "encoder = encoder.cuda(0)\n",
    "decoder = decoder.cuda(0)\n",
    "\n",
    "early_stop = 2\n",
    "best_blue_score = -1\n",
    "best_index = -1\n",
    "\n",
    "save_model_dir_name = '../save_model/vi_to_en_'\n",
    "\n",
    "para_list_1, para_list_2 = parameters_list(encoder, decoder)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([{'params': para_list_1, 'lr': 0.001},\n",
    "                              {'params': para_list_2, 'lr': 0.001}])\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for index_unique in range(100):\n",
    "    train(encoder, decoder, optimizer, train_vi_en_iter, teacher_forcing_ratio)\n",
    "    blue_score = test(encoder, decoder, validation_vi_en_iter)\n",
    "    print('epoch: ',index_unique, ' the blue score on validation dataset is : ', blue_score)\n",
    "    sys.stdout.flush()\n",
    "    if best_blue_score < blue_score:\n",
    "        \n",
    "        best_index = index_unique\n",
    "        best_blue_score = blue_score\n",
    "        best_encoder = encoder\n",
    "        best_decoder = decoder\n",
    "        torch.save(encoder, save_model_dir_name+'encode_'+str(index_unique))\n",
    "        torch.save(decoder, save_model_dir_name+'decoder_'+str(index_unique))\n",
    "        \n",
    "    if index_unique - best_index >= early_stop:\n",
    "        break\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "print('--------------------------------------')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "encoder = best_encoder\n",
    "decoder = best_decoder\n",
    "        \n",
    "        \n",
    "\n",
    "para_list = parameters_list_change_grad(encoder, decoder)     \n",
    "optimizer = torch.optim.Adam(para_list, lr = 0.001)  \n",
    "save_model_dir_name = '../save_model/refined_vi_to_en_'\n",
    "\n",
    "early_stop = 2\n",
    "best_blue_score = -1\n",
    "best_index = -1\n",
    "\n",
    "for index_unique in range(100):\n",
    "    train(encoder, decoder, optimizer, train_vi_en_iter, teacher_forcing_ratio)\n",
    "    blue_score = test(encoder, decoder, validation_vi_en_iter)\n",
    "    print('epoch: ',index_unique, ' the blue score on validation dataset is : ', blue_score)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if best_blue_score < blue_score:\n",
    "        \n",
    "        best_index = index_unique\n",
    "        best_blue_score = blue_score\n",
    "        torch.save(encoder, save_model_dir_name+'encode_'+str(index_unique))\n",
    "        torch.save(decoder, save_model_dir_name+'decoder_'+str(index_unique))\n",
    "    if index_unique - best_index >= early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' UNK ',\n",
       " '<pad>',\n",
       " '<sos>',\n",
       " '<eos>',\n",
       " ',',\n",
       " '.',\n",
       " 'the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " 'that',\n",
       " 'i',\n",
       " 'in',\n",
       " 'it',\n",
       " 'you',\n",
       " 'we',\n",
       " 'is',\n",
       " '&apos;s',\n",
       " 'this',\n",
       " 'so',\n",
       " '&quot;',\n",
       " 'they',\n",
       " 'for',\n",
       " 'was',\n",
       " 'are',\n",
       " 'have',\n",
       " 'what',\n",
       " 'but',\n",
       " 'on',\n",
       " 'with',\n",
       " 'can',\n",
       " '--',\n",
       " '?',\n",
       " '&apos;t',\n",
       " 'about',\n",
       " 'there',\n",
       " 'be',\n",
       " 'as',\n",
       " 'all',\n",
       " 'at',\n",
       " 'not',\n",
       " 'my',\n",
       " 'do',\n",
       " 'one',\n",
       " 'people',\n",
       " '&apos;re',\n",
       " 'from',\n",
       " 'like',\n",
       " 'if',\n",
       " 'now',\n",
       " 'an',\n",
       " 'he',\n",
       " 'our',\n",
       " 'these',\n",
       " 'just',\n",
       " ':',\n",
       " 'when',\n",
       " 'or',\n",
       " 'because',\n",
       " 'how',\n",
       " 'me',\n",
       " 'very',\n",
       " 'by',\n",
       " 'out',\n",
       " 'them',\n",
       " 'more',\n",
       " 'going',\n",
       " 'up',\n",
       " 'know',\n",
       " 'your',\n",
       " 'who',\n",
       " 'had',\n",
       " 'think',\n",
       " 'their',\n",
       " 'which',\n",
       " 'see',\n",
       " 'were',\n",
       " 'would',\n",
       " 'really',\n",
       " 'here',\n",
       " 'get',\n",
       " 'then',\n",
       " 'us',\n",
       " 'world',\n",
       " '&apos;ve',\n",
       " '&apos;m',\n",
       " 'some',\n",
       " 'time',\n",
       " 'actually',\n",
       " 'don',\n",
       " 'has',\n",
       " 'way',\n",
       " 'into',\n",
       " 'years',\n",
       " 'will',\n",
       " 'things',\n",
       " 'where',\n",
       " 'other',\n",
       " 'no',\n",
       " 'could',\n",
       " 'want',\n",
       " 'go',\n",
       " 'make',\n",
       " 'she',\n",
       " 'well',\n",
       " 'been',\n",
       " 'said',\n",
       " 'first',\n",
       " 'something',\n",
       " 'right',\n",
       " 'two',\n",
       " 'than',\n",
       " 'those',\n",
       " 'much',\n",
       " 'also',\n",
       " 'new',\n",
       " 'look',\n",
       " 'his',\n",
       " 'thing',\n",
       " 'life',\n",
       " 'little',\n",
       " 'back',\n",
       " 'most',\n",
       " 'say',\n",
       " 'even',\n",
       " 'got',\n",
       " 'over',\n",
       " 'only',\n",
       " 'work',\n",
       " 'need',\n",
       " 'many',\n",
       " 'why',\n",
       " 'take',\n",
       " 'did',\n",
       " ';',\n",
       " 'good',\n",
       " 'lot',\n",
       " 'around',\n",
       " 'her',\n",
       " 'different',\n",
       " 'every',\n",
       " 'kind',\n",
       " 'through',\n",
       " 'let',\n",
       " '&apos;ll',\n",
       " 'down',\n",
       " 'same',\n",
       " 'come',\n",
       " 'being',\n",
       " 'use',\n",
       " 'day',\n",
       " 'doing',\n",
       " 'any',\n",
       " 'put',\n",
       " 'talk',\n",
       " 'three',\n",
       " 'thank',\n",
       " 'today',\n",
       " 'called',\n",
       " '&apos;d',\n",
       " 'percent',\n",
       " 'made',\n",
       " 'tell',\n",
       " 'after',\n",
       " 'find',\n",
       " 'own',\n",
       " 'great',\n",
       " 'change',\n",
       " 'fact',\n",
       " 'year',\n",
       " 'didn',\n",
       " 'big',\n",
       " 'brain',\n",
       " 'idea',\n",
       " 'should',\n",
       " 'human',\n",
       " 'last',\n",
       " 'started',\n",
       " 'before',\n",
       " 'its',\n",
       " 'went',\n",
       " 'give',\n",
       " 'another',\n",
       " 'thought',\n",
       " 'important',\n",
       " 'might',\n",
       " 'never',\n",
       " 'problem',\n",
       " 'better',\n",
       " 'still',\n",
       " 'course',\n",
       " 'each',\n",
       " 'show',\n",
       " 'able',\n",
       " 'story',\n",
       " 'part',\n",
       " 'next',\n",
       " 'again',\n",
       " 'does',\n",
       " 'together',\n",
       " 'came',\n",
       " '&apos;',\n",
       " 'off',\n",
       " 'system',\n",
       " 'start',\n",
       " 'between',\n",
       " 'women',\n",
       " 'love',\n",
       " 'few',\n",
       " 'ago',\n",
       " 'him',\n",
       " 'question',\n",
       " 'used',\n",
       " 'bit',\n",
       " 'place',\n",
       " 'data',\n",
       " 'too',\n",
       " 'found',\n",
       " 'maybe',\n",
       " 'school',\n",
       " 'mean',\n",
       " 'example',\n",
       " 'end',\n",
       " 'done',\n",
       " 'technology',\n",
       " 'understand',\n",
       " 'doesn',\n",
       " 'live',\n",
       " 'long',\n",
       " 'children',\n",
       " 'wanted',\n",
       " 'looking',\n",
       " 'believe',\n",
       " 'may',\n",
       " 'call',\n",
       " 'always',\n",
       " 'water',\n",
       " 'money',\n",
       " 'real',\n",
       " 'four',\n",
       " 'point',\n",
       " 'working',\n",
       " 'ever',\n",
       " 'person',\n",
       " 'help',\n",
       " 'information',\n",
       " 'sort',\n",
       " 'whole',\n",
       " 'kids',\n",
       " 'away',\n",
       " 'feel',\n",
       " 'country',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'everything',\n",
       " 'using',\n",
       " 'create',\n",
       " '10',\n",
       " 'means',\n",
       " 'food',\n",
       " 'second',\n",
       " 'power',\n",
       " 'home',\n",
       " 'become',\n",
       " 'five',\n",
       " 'million',\n",
       " 'space',\n",
       " 'number',\n",
       " 'took',\n",
       " 'thinking',\n",
       " 'times',\n",
       " 'quite',\n",
       " 'less',\n",
       " 'future',\n",
       " 'says',\n",
       " 'best',\n",
       " 'social',\n",
       " '!',\n",
       " 'old',\n",
       " 'small',\n",
       " 'enough',\n",
       " 'design',\n",
       " 'light',\n",
       " 'getting',\n",
       " 'comes',\n",
       " 'lives',\n",
       " 'making',\n",
       " 'energy',\n",
       " 'man',\n",
       " 'am',\n",
       " 'without',\n",
       " 'ask',\n",
       " 'body',\n",
       " 'okay',\n",
       " 'probably',\n",
       " 'science',\n",
       " 'left',\n",
       " 'sense',\n",
       " 'happened',\n",
       " 'countries',\n",
       " 'makes',\n",
       " 'talking',\n",
       " 'pretty',\n",
       " 'while',\n",
       " 'yet',\n",
       " 'city',\n",
       " 'dollars',\n",
       " 'imagine',\n",
       " 'experience',\n",
       " 'told',\n",
       " 'learn',\n",
       " 'such',\n",
       " 'simple',\n",
       " 'across',\n",
       " 'interesting',\n",
       " 'moment',\n",
       " 'happen',\n",
       " 'cells',\n",
       " 'once',\n",
       " 'build',\n",
       " 'case',\n",
       " 'living',\n",
       " 'inside',\n",
       " 'building',\n",
       " 'room',\n",
       " 'asked',\n",
       " 'men',\n",
       " 'saw',\n",
       " 'ways',\n",
       " 'anything',\n",
       " 'health',\n",
       " 'stuff',\n",
       " 'family',\n",
       " 'hard',\n",
       " 'else',\n",
       " 'later',\n",
       " 'coming',\n",
       " 'happens',\n",
       " 'goes',\n",
       " 'half',\n",
       " 'project',\n",
       " '&#91;',\n",
       " '&#93;',\n",
       " 'process',\n",
       " 'music',\n",
       " 'reason',\n",
       " 'someone',\n",
       " 'within',\n",
       " 'having',\n",
       " 'high',\n",
       " 'problems',\n",
       " 'business',\n",
       " 'side',\n",
       " 'whether',\n",
       " 'days',\n",
       " 'wrong',\n",
       " 'nothing',\n",
       " 'states',\n",
       " 'young',\n",
       " 'earth',\n",
       " 'single',\n",
       " 'bad',\n",
       " 'mind',\n",
       " 'often',\n",
       " 'book',\n",
       " 'words',\n",
       " 'care',\n",
       " 'seen',\n",
       " 'myself',\n",
       " 'everybody',\n",
       " 'basically',\n",
       " 'move',\n",
       " 'play',\n",
       " 'saying',\n",
       " 'global',\n",
       " 'both',\n",
       " 'ideas',\n",
       " 'far',\n",
       " 'picture',\n",
       " 'looked',\n",
       " 'almost',\n",
       " 'hand',\n",
       " 'keep',\n",
       " 'language',\n",
       " '20',\n",
       " 'possible',\n",
       " 'true',\n",
       " 'billion',\n",
       " 'video',\n",
       " 'remember',\n",
       " 'computer',\n",
       " 'answer',\n",
       " 'public',\n",
       " 'child',\n",
       " 'community',\n",
       " 'research',\n",
       " 'control',\n",
       " 'oh',\n",
       " 'matter',\n",
       " 'africa',\n",
       " 'hope',\n",
       " 'age',\n",
       " 'top',\n",
       " 'set',\n",
       " 'history',\n",
       " 'yes',\n",
       " 'art',\n",
       " 'six',\n",
       " 'sure',\n",
       " 'sometimes',\n",
       " 'form',\n",
       " 'already',\n",
       " 'heard',\n",
       " 'war',\n",
       " 'bring',\n",
       " 'looks',\n",
       " 'until',\n",
       " 'instead',\n",
       " 'guy',\n",
       " 'stories',\n",
       " 'face',\n",
       " 'amazing',\n",
       " 'decided',\n",
       " 'united',\n",
       " 'job',\n",
       " 'works',\n",
       " 'read',\n",
       " 'education',\n",
       " 'group',\n",
       " 'built',\n",
       " '&amp;',\n",
       " 'beautiful',\n",
       " 'open',\n",
       " 'since',\n",
       " 'society',\n",
       " 'months',\n",
       " 'wasn',\n",
       " 'couple',\n",
       " 'car',\n",
       " 'everyone',\n",
       " 'piece',\n",
       " 'taking',\n",
       " 'ted',\n",
       " 'under',\n",
       " 'government',\n",
       " 'though',\n",
       " 'minutes',\n",
       " 'nature',\n",
       " 'friends',\n",
       " 'somebody',\n",
       " 'became',\n",
       " 'heart',\n",
       " 'isn',\n",
       " 'order',\n",
       " 'share',\n",
       " 'universe',\n",
       " 'run',\n",
       " 'woman',\n",
       " 'internet',\n",
       " 'word',\n",
       " 'exactly',\n",
       " 'turn',\n",
       " 'learned',\n",
       " 'middle',\n",
       " 'cancer',\n",
       " 'themselves',\n",
       " 'huge',\n",
       " 'night',\n",
       " 'created',\n",
       " 'completely',\n",
       " '30',\n",
       " 'gets',\n",
       " 'china',\n",
       " 'state',\n",
       " 'study',\n",
       " 'planet',\n",
       " '',\n",
       " 'happening',\n",
       " 'stop',\n",
       " 'questions',\n",
       " 'knew',\n",
       " 'mother',\n",
       " 'hear',\n",
       " 'hours',\n",
       " 'against',\n",
       " 'environment',\n",
       " 'others',\n",
       " 'places',\n",
       " 'model',\n",
       " 'rather',\n",
       " 'kinds',\n",
       " 'itself',\n",
       " 'students',\n",
       " 'shows',\n",
       " 'yeah',\n",
       " 'game',\n",
       " 'ourselves',\n",
       " '...',\n",
       " 'news',\n",
       " 'difference',\n",
       " 'head',\n",
       " 'house',\n",
       " 'line',\n",
       " 'ones',\n",
       " 'god',\n",
       " 'company',\n",
       " 'past',\n",
       " 'name',\n",
       " 'gave',\n",
       " 'worked',\n",
       " 'sound',\n",
       " 'thousands',\n",
       " 'must',\n",
       " 'couldn',\n",
       " '50',\n",
       " 'finally',\n",
       " 'large',\n",
       " 'entire',\n",
       " 'happy',\n",
       " 'turns',\n",
       " 'won',\n",
       " 'during',\n",
       " 'third',\n",
       " 'learning',\n",
       " 'disease',\n",
       " 'india',\n",
       " 'level',\n",
       " 'particular',\n",
       " 'perhaps',\n",
       " 'front',\n",
       " 'america',\n",
       " 'outside',\n",
       " 'american',\n",
       " 'early',\n",
       " 'powerful',\n",
       " 'changed',\n",
       " 'takes',\n",
       " 'machine',\n",
       " 'population',\n",
       " 'lots',\n",
       " 'systems',\n",
       " 'figure',\n",
       " 'guys',\n",
       " 'species',\n",
       " '100',\n",
       " 'along',\n",
       " 'audience',\n",
       " 'given',\n",
       " 'least',\n",
       " 'spend',\n",
       " 'natural',\n",
       " 'animals',\n",
       " 'behind',\n",
       " 'culture',\n",
       " 'u.s.',\n",
       " 'yourself',\n",
       " 'cell',\n",
       " 'companies',\n",
       " 'easy',\n",
       " 'realized',\n",
       " 'oil',\n",
       " 'simply',\n",
       " 'team',\n",
       " 'media',\n",
       " 'air',\n",
       " 'difficult',\n",
       " 'per',\n",
       " 'needed',\n",
       " 'free',\n",
       " 'needs',\n",
       " 'image',\n",
       " 'leave',\n",
       " 'black',\n",
       " 'economic',\n",
       " 'century',\n",
       " 'close',\n",
       " 'father',\n",
       " 'grow',\n",
       " 'paper',\n",
       " 'political',\n",
       " 'eyes',\n",
       " 'walk',\n",
       " 'area',\n",
       " 'felt',\n",
       " 'seven',\n",
       " 'full',\n",
       " 'turned',\n",
       " 'local',\n",
       " 'certain',\n",
       " 'moving',\n",
       " 'seeing',\n",
       " 'value',\n",
       " 'spent',\n",
       " 'taken',\n",
       " 'cannot',\n",
       " 'eat',\n",
       " 'hands',\n",
       " 'death',\n",
       " 'girl',\n",
       " 'parts',\n",
       " 'green',\n",
       " 'wonderful',\n",
       " 'beginning',\n",
       " 'whatever',\n",
       " 'cities',\n",
       " 'longer',\n",
       " 'voice',\n",
       " 'began',\n",
       " 'quickly',\n",
       " 'growth',\n",
       " 'parents',\n",
       " 'view',\n",
       " 'amount',\n",
       " 'size',\n",
       " 'speak',\n",
       " 'behavior',\n",
       " 'terms',\n",
       " 'lost',\n",
       " 'stage',\n",
       " 'york',\n",
       " 'step',\n",
       " 'watch',\n",
       " '15',\n",
       " 'morning',\n",
       " 'center',\n",
       " 'wouldn',\n",
       " 'complex',\n",
       " 'week',\n",
       " 'common',\n",
       " 'images',\n",
       " 'tried',\n",
       " 'write',\n",
       " 'either',\n",
       " 'friend',\n",
       " 'buy',\n",
       " 'eight',\n",
       " 'rest',\n",
       " 'test',\n",
       " 'lab',\n",
       " 'attention',\n",
       " 'deal',\n",
       " 'average',\n",
       " 'sitting',\n",
       " 'national',\n",
       " 'street',\n",
       " 'born',\n",
       " 'cost',\n",
       " 'fear',\n",
       " 'pay',\n",
       " 'weeks',\n",
       " 'giving',\n",
       " 'knowledge',\n",
       " 'reality',\n",
       " 'blue',\n",
       " 'feeling',\n",
       " 'personal',\n",
       " 'poor',\n",
       " 'based',\n",
       " 'growing',\n",
       " 'opportunity',\n",
       " 'changes',\n",
       " 'brought',\n",
       " 'red',\n",
       " 'anyone',\n",
       " 'books',\n",
       " 'dna',\n",
       " 'economy',\n",
       " 'scientists',\n",
       " 'animal',\n",
       " 'film',\n",
       " 'gone',\n",
       " 'interested',\n",
       " 'physical',\n",
       " 'seems',\n",
       " 'south',\n",
       " 'ability',\n",
       " 'english',\n",
       " 'ground',\n",
       " 'humans',\n",
       " 'save',\n",
       " 'climate',\n",
       " 'developed',\n",
       " 'tools',\n",
       " 'developing',\n",
       " 'known',\n",
       " 'act',\n",
       " 'challenge',\n",
       " 'die',\n",
       " 'field',\n",
       " 'incredible',\n",
       " 'individual',\n",
       " 'millions',\n",
       " 'running',\n",
       " 'girls',\n",
       " 'realize',\n",
       " 'short',\n",
       " '/',\n",
       " 'access',\n",
       " 'land',\n",
       " 'online',\n",
       " 'starting',\n",
       " 'understanding',\n",
       " 'issue',\n",
       " 'numbers',\n",
       " 'structure',\n",
       " 'wrote',\n",
       " 'ice',\n",
       " 'creating',\n",
       " 'material',\n",
       " 'white',\n",
       " 'baby',\n",
       " 'playing',\n",
       " 'focus',\n",
       " 'fun',\n",
       " 'key',\n",
       " 'innovation',\n",
       " 'literally',\n",
       " 'market',\n",
       " 'met',\n",
       " 'chinese',\n",
       " 'hundreds',\n",
       " 'writing',\n",
       " 'nice',\n",
       " 'scale',\n",
       " 'solution',\n",
       " 'hold',\n",
       " 'knows',\n",
       " 'modern',\n",
       " 'ocean',\n",
       " 'alone',\n",
       " 'absolutely',\n",
       " 'especially',\n",
       " 'network',\n",
       " 'areas',\n",
       " 'patients',\n",
       " 'seem',\n",
       " 'stand',\n",
       " 'telling',\n",
       " 'asking',\n",
       " 'product',\n",
       " 'program',\n",
       " 'gives',\n",
       " 'sun',\n",
       " 'university',\n",
       " 'dead',\n",
       " 'development',\n",
       " 'teach',\n",
       " '40',\n",
       " 'movement',\n",
       " 'type',\n",
       " 'force',\n",
       " 'cool',\n",
       " 'experiment',\n",
       " 'patient',\n",
       " 'rate',\n",
       " 'solve',\n",
       " 'theory',\n",
       " 'nobody',\n",
       " 'generation',\n",
       " 'result',\n",
       " 'risk',\n",
       " 'pictures',\n",
       " 'showed',\n",
       " 'clear',\n",
       " 'discovered',\n",
       " 'aren',\n",
       " 'fly',\n",
       " 'forward',\n",
       " 'meet',\n",
       " 'phone',\n",
       " 'changing',\n",
       " 'normal',\n",
       " 'ok',\n",
       " 'success',\n",
       " 'teachers',\n",
       " 'bottom',\n",
       " 'kid',\n",
       " 'blood',\n",
       " 'cars',\n",
       " 'industry',\n",
       " 'soon',\n",
       " 'feet',\n",
       " 'incredibly',\n",
       " 'produce',\n",
       " 'box',\n",
       " 'games',\n",
       " 'materials',\n",
       " 'sounds',\n",
       " 'deep',\n",
       " 'indeed',\n",
       " 'sleep',\n",
       " 'east',\n",
       " 'impact',\n",
       " 'obviously',\n",
       " 'potential',\n",
       " 'resources',\n",
       " 'sex',\n",
       " 'support',\n",
       " 'talked',\n",
       " 'explain',\n",
       " 'recently',\n",
       " 'beyond',\n",
       " 'general',\n",
       " 'haven',\n",
       " 'message',\n",
       " 'happiness',\n",
       " 'office',\n",
       " 'listen',\n",
       " 'relationship',\n",
       " 'surface',\n",
       " 'fast',\n",
       " 'chance',\n",
       " 'eye',\n",
       " 'involved',\n",
       " 'law',\n",
       " 'revolution',\n",
       " 'democracy',\n",
       " 'issues',\n",
       " 'present',\n",
       " 'schools',\n",
       " 'truth',\n",
       " 'begin',\n",
       " 'choice',\n",
       " 'groups',\n",
       " 'stay',\n",
       " 'digital',\n",
       " 'tiny',\n",
       " 'cut',\n",
       " 'guess',\n",
       " 'major',\n",
       " 'send',\n",
       " 'technologies',\n",
       " 'boy',\n",
       " 'color',\n",
       " 'please',\n",
       " 'available',\n",
       " 'cause',\n",
       " 'google',\n",
       " 'led',\n",
       " 'map',\n",
       " 'rules',\n",
       " 'allow',\n",
       " 'europe',\n",
       " 'medical',\n",
       " 'north',\n",
       " 'peace',\n",
       " 'journey',\n",
       " 'situation',\n",
       " 'west',\n",
       " 'hundred',\n",
       " 'projects',\n",
       " 'hit',\n",
       " 'device',\n",
       " 'shape',\n",
       " 'walking',\n",
       " 'everywhere',\n",
       " 'memory',\n",
       " 'robot',\n",
       " 'individuals',\n",
       " 'wait',\n",
       " 'creative',\n",
       " 'miles',\n",
       " 'putting',\n",
       " 'suddenly',\n",
       " 'dark',\n",
       " 'becomes',\n",
       " 'likely',\n",
       " 'approach',\n",
       " 'tells',\n",
       " 'bigger',\n",
       " 'died',\n",
       " 'similar',\n",
       " 'table',\n",
       " 'certainly',\n",
       " 'teacher',\n",
       " 'designed',\n",
       " 'smart',\n",
       " 'special',\n",
       " 'effect',\n",
       " 'evidence',\n",
       " 'reasons',\n",
       " 'results',\n",
       " 'vision',\n",
       " 'anybody',\n",
       " 'crazy',\n",
       " 'add',\n",
       " 'code',\n",
       " 'computers',\n",
       " 'towards',\n",
       " 'perfect',\n",
       " 'ready',\n",
       " 'scientific',\n",
       " 'son',\n",
       " 'action',\n",
       " 'favorite',\n",
       " 'fight',\n",
       " 'lose',\n",
       " 'software',\n",
       " 'basic',\n",
       " 'carbon',\n",
       " 'develop',\n",
       " 'lived',\n",
       " 'sit',\n",
       " 'month',\n",
       " 'notice',\n",
       " 'wall',\n",
       " '12',\n",
       " 'college',\n",
       " 'drive',\n",
       " 'leaders',\n",
       " 'sea',\n",
       " 'starts',\n",
       " 'examples',\n",
       " 'gt',\n",
       " 'measure',\n",
       " 'strong',\n",
       " 'violence',\n",
       " 'lt',\n",
       " 'meaning',\n",
       " 'moral',\n",
       " 'biggest',\n",
       " 'drug',\n",
       " 'several',\n",
       " 'taught',\n",
       " 'fall',\n",
       " 'moved',\n",
       " 'onto',\n",
       " 'particularly',\n",
       " 'choose',\n",
       " 'dream',\n",
       " 'freedom',\n",
       " 'showing',\n",
       " 'waste',\n",
       " 'eventually',\n",
       " 'faster',\n",
       " 'plan',\n",
       " 'quality',\n",
       " 'song',\n",
       " 'spread',\n",
       " 'trust',\n",
       " 'camera',\n",
       " 'further',\n",
       " 'hospital',\n",
       " 'student',\n",
       " 'communities',\n",
       " 'road',\n",
       " 'worth',\n",
       " 'break',\n",
       " 'drugs',\n",
       " 'jobs',\n",
       " 'em',\n",
       " 'nine',\n",
       " 'totally',\n",
       " 'decisions',\n",
       " 'response',\n",
       " 'follow',\n",
       " 'region',\n",
       " 'worse',\n",
       " 'brains',\n",
       " 'compassion',\n",
       " 'low',\n",
       " '25',\n",
       " 'reach',\n",
       " 'lead',\n",
       " 'rich',\n",
       " 'village',\n",
       " 'tool',\n",
       " 'watching',\n",
       " 'content',\n",
       " 'mental',\n",
       " 'mom',\n",
       " 'training',\n",
       " 'pain',\n",
       " 'studies',\n",
       " 'among',\n",
       " 'connected',\n",
       " 'fish',\n",
       " 'international',\n",
       " 'mine',\n",
       " 'wants',\n",
       " 'wish',\n",
       " 'grew',\n",
       " 'including',\n",
       " 'safe',\n",
       " 'shown',\n",
       " 'traditional',\n",
       " 'allowed',\n",
       " 'buildings',\n",
       " 'crisis',\n",
       " 'solar',\n",
       " 'trees',\n",
       " 'web',\n",
       " 'artist',\n",
       " 'bodies',\n",
       " 'meant',\n",
       " 'tv',\n",
       " 'class',\n",
       " 'activity',\n",
       " 'continue',\n",
       " ...]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_en.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_prob_value_list, k_word_index_list = p.topk(10,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n",
      "<class 'numpy.float32'> <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(list(k_prob_value_list), list(k_word_index_list)):\n",
    "    print(type(a), type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_Decoder_With_Attention(100,100,100)\n",
    "model.init_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_0 torch.Size([1, 256])\n",
      "c_0 torch.Size([1, 256])\n",
      "h_1 torch.Size([1, 256])\n",
      "c_1 torch.Size([1, 256])\n",
      "embedding_layer.weight torch.Size([47818, 256])\n",
      "lstm.weight_ih_l0 torch.Size([1024, 256])\n",
      "lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0 torch.Size([1024])\n",
      "lstm.bias_hh_l0 torch.Size([1024])\n",
      "lstm.weight_ih_l0_reverse torch.Size([1024, 256])\n",
      "lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "lstm.bias_hh_l0_reverse torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for i,j in model.named_parameters():\n",
    "    print(i,j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bi_Multi_Layer_LSTM_Encoder(40000)\n",
    "model = model.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, h_n, c_n = model(batch.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0644, -0.2107,  0.0619, -0.1083,  0.1643, -0.1319, -0.1833,\n",
       "          -0.0865, -0.1231,  0.0314,  0.1980,  0.0591,  0.0410, -0.0407,\n",
       "           0.1086,  0.1122,  0.0773, -0.1008, -0.0328,  0.0581, -0.1446,\n",
       "           0.1377, -0.2802, -0.1474, -0.1325, -0.1730, -0.0371,  0.0442,\n",
       "           0.0064,  0.0894,  0.0054, -0.1113, -0.0061,  0.0448,  0.0152,\n",
       "           0.0657, -0.0109, -0.0666,  0.1466, -0.0901,  0.2611, -0.3704,\n",
       "          -0.1226,  0.0712,  0.1101,  0.1109, -0.2114,  0.1134, -0.0082,\n",
       "          -0.0465,  0.2074,  0.0117, -0.1715,  0.3426, -0.2799, -0.0629,\n",
       "           0.0599, -0.1153,  0.1230,  0.2035, -0.0557, -0.1402, -0.0621,\n",
       "          -0.0943,  0.0436, -0.0852, -0.1129, -0.0263, -0.1597,  0.2120,\n",
       "           0.0877,  0.1135, -0.0120,  0.2677, -0.0315,  0.3531,  0.0118,\n",
       "           0.1171,  0.1543,  0.0550,  0.0083,  0.0208,  0.0730, -0.0727,\n",
       "          -0.0326, -0.1360,  0.0260,  0.0334, -0.1456,  0.0079, -0.0447,\n",
       "           0.1556,  0.2574, -0.0306, -0.1912, -0.3019,  0.0219, -0.0611,\n",
       "          -0.0219,  0.2502, -0.0353, -0.0911,  0.0493, -0.0956, -0.1474,\n",
       "           0.0351,  0.1162,  0.0350, -0.0600, -0.0069,  0.1656,  0.0876,\n",
       "          -0.0420,  0.0775,  0.2911,  0.2049,  0.0989,  0.1300,  0.1716,\n",
       "           0.0820,  0.0387, -0.1695, -0.0509, -0.0024,  0.1895,  0.0036,\n",
       "          -0.0985, -0.0196, -0.1084,  0.1585, -0.0905,  0.0838, -0.0677,\n",
       "           0.0551,  0.1476,  0.1471,  0.0002,  0.0525,  0.0533,  0.2657,\n",
       "          -0.2636, -0.0530,  0.2672,  0.2025,  0.1236,  0.0089, -0.1380,\n",
       "          -0.0552,  0.0535,  0.1386,  0.1019, -0.1520,  0.1597, -0.3510,\n",
       "           0.0505,  0.0427, -0.0356, -0.0628, -0.0532, -0.1775,  0.1507,\n",
       "          -0.1162,  0.2582, -0.0932,  0.0740,  0.1321, -0.1084,  0.0446,\n",
       "           0.1638, -0.1996,  0.0535,  0.1565, -0.1671,  0.1238, -0.2131,\n",
       "           0.3230,  0.0859,  0.1385, -0.0063, -0.1708,  0.0881,  0.1498,\n",
       "           0.0967,  0.1500, -0.0391,  0.0118,  0.1642, -0.2105, -0.1583,\n",
       "          -0.0466, -0.0367, -0.2311, -0.0806, -0.0417,  0.1497,  0.0066,\n",
       "           0.0180, -0.1929,  0.0837, -0.0591, -0.0511,  0.1522, -0.0218,\n",
       "          -0.0786, -0.0801, -0.0857,  0.3593,  0.0272,  0.1305, -0.0217,\n",
       "          -0.0268, -0.1422,  0.0242,  0.0905,  0.0088,  0.0612, -0.1834,\n",
       "           0.1262,  0.1676,  0.1410,  0.1608,  0.0899, -0.0569, -0.1285,\n",
       "           0.0776,  0.0105,  0.0487,  0.2724,  0.0414, -0.1227, -0.2782,\n",
       "          -0.1736,  0.1371, -0.0034, -0.3249,  0.0642,  0.0650, -0.2383,\n",
       "          -0.2454, -0.2218,  0.0036, -0.0699,  0.0969,  0.1433, -0.1372,\n",
       "          -0.0760, -0.0015, -0.1263,  0.1785, -0.0894, -0.1273,  0.0591,\n",
       "          -0.0411, -0.0444,  0.0860, -0.0839]]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0644, -0.2107,  0.0619, -0.1083,  0.1643, -0.1319, -0.1833, -0.0865,\n",
       "        -0.1231,  0.0314,  0.1980,  0.0591,  0.0410, -0.0407,  0.1086,  0.1122,\n",
       "         0.0773, -0.1008, -0.0328,  0.0581, -0.1446,  0.1377, -0.2802, -0.1474,\n",
       "        -0.1325, -0.1730, -0.0371,  0.0442,  0.0064,  0.0894,  0.0054, -0.1113,\n",
       "        -0.0061,  0.0448,  0.0152,  0.0657, -0.0109, -0.0666,  0.1466, -0.0901,\n",
       "         0.2611, -0.3704, -0.1226,  0.0712,  0.1101,  0.1109, -0.2114,  0.1134,\n",
       "        -0.0082, -0.0465,  0.2074,  0.0117, -0.1715,  0.3426, -0.2799, -0.0629,\n",
       "         0.0599, -0.1153,  0.1230,  0.2035, -0.0557, -0.1402, -0.0621, -0.0943,\n",
       "         0.0436, -0.0852, -0.1129, -0.0263, -0.1597,  0.2120,  0.0877,  0.1135,\n",
       "        -0.0120,  0.2677, -0.0315,  0.3531,  0.0118,  0.1171,  0.1543,  0.0550,\n",
       "         0.0083,  0.0208,  0.0730, -0.0727, -0.0326, -0.1360,  0.0260,  0.0334,\n",
       "        -0.1456,  0.0079, -0.0447,  0.1556,  0.2574, -0.0306, -0.1912, -0.3019,\n",
       "         0.0219, -0.0611, -0.0219,  0.2502, -0.0353, -0.0911,  0.0493, -0.0956,\n",
       "        -0.1474,  0.0351,  0.1162,  0.0350, -0.0600, -0.0069,  0.1656,  0.0876,\n",
       "        -0.0420,  0.0775,  0.2911,  0.2049,  0.0989,  0.1300,  0.1716,  0.0820,\n",
       "         0.0387, -0.1695, -0.0509, -0.0024,  0.1895,  0.0036, -0.0985, -0.0196,\n",
       "        -0.1084,  0.1585, -0.0905,  0.0838, -0.0677,  0.0551,  0.1476,  0.1471,\n",
       "         0.0002,  0.0525,  0.0533,  0.2657, -0.2636, -0.0530,  0.2672,  0.2025,\n",
       "         0.1236,  0.0089, -0.1380, -0.0552,  0.0535,  0.1386,  0.1019, -0.1520,\n",
       "         0.1597, -0.3510,  0.0505,  0.0427, -0.0356, -0.0628, -0.0532, -0.1775,\n",
       "         0.1507, -0.1162,  0.2582, -0.0932,  0.0740,  0.1321, -0.1084,  0.0446,\n",
       "         0.1638, -0.1996,  0.0535,  0.1565, -0.1671,  0.1238, -0.2131,  0.3230,\n",
       "         0.0859,  0.1385, -0.0063, -0.1708,  0.0881,  0.1498,  0.0967,  0.1500,\n",
       "        -0.0391,  0.0118,  0.1642, -0.2105, -0.1583, -0.0466, -0.0367, -0.2311,\n",
       "        -0.0806, -0.0417,  0.1497,  0.0066,  0.0180, -0.1929,  0.0837, -0.0591,\n",
       "        -0.0511,  0.1522, -0.0218, -0.0786, -0.0801, -0.0857,  0.3593,  0.0272,\n",
       "         0.1305, -0.0217, -0.0268, -0.1422,  0.0242,  0.0905,  0.0088,  0.0612,\n",
       "        -0.1834,  0.1262,  0.1676,  0.1410,  0.1608,  0.0899, -0.0569, -0.1285,\n",
       "         0.0776,  0.0105,  0.0487,  0.2724,  0.0414, -0.1227, -0.2782, -0.1736,\n",
       "         0.1371, -0.0034, -0.3249,  0.0642,  0.0650, -0.2383, -0.2454, -0.2218,\n",
       "         0.0036, -0.0699,  0.0969,  0.1433, -0.1372, -0.0760, -0.0015, -0.1263,\n",
       "         0.1785, -0.0894, -0.1273,  0.0591, -0.0411, -0.0444,  0.0860, -0.0839],\n",
       "       device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,0,256:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/ipykernel_launcher.py:137: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    }
   ],
   "source": [
    "p = test(encoder,decoder, train_vi_en_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47818])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-08a15847d1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "p.topk(10)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.0558, -2.2132, -2.9957, -3.2576, -3.3534, -3.4411, -3.5530, -3.7668,\n",
       "          -3.7925, -3.8231]], device='cuda:0', grad_fn=<TopkBackward>),\n",
       " tensor([[15,  6, 20, 14,  7, 42, 70, 13, 12, 50]], device='cuda:0'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEXT_vi = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = True, tokenize= lambda t:t.split(),\n",
    "                                        include_lengths=True)\n",
    "TEXT_en = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = True, tokenize= lambda t:t.split(),\n",
    "                              lower=True, init_token='<sos>', eos_token='<eos>',include_lengths=True)\n",
    "\n",
    "\n",
    "train_vi_en = torchtext.data.TabularDataset('../data/processed_data/train_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "validation_vi_en = torchtext.data.TabularDataset('../data/processed_data/dev_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "\n",
    "\n",
    "TEXT_vi.build_vocab(train_vi_en)\n",
    "TEXT_en.build_vocab(train_vi_en)\n",
    "\n",
    "\n",
    "train_vi_en_iter = torchtext.data.BucketIterator(train_vi_en, batch_size=1, sort_key= lambda e: len(e.source) + len(e.target),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source) + len(e.target),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, sys\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "TEXT_vi = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = True, tokenize= lambda t:t.split(),\n",
    "                                        include_lengths=True)\n",
    "TEXT_en = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = True, tokenize= lambda t:t.split(),\n",
    "                              lower=True, init_token='<sos>', eos_token='<eos>',include_lengths=True)\n",
    "\n",
    "\n",
    "train_vi_en = torchtext.data.TabularDataset('../data/processed_data/train_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "validation_vi_en = torchtext.data.TabularDataset('../data/processed_data/dev_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "\n",
    "\n",
    "TEXT_vi.build_vocab(train_vi_en)\n",
    "TEXT_en.build_vocab(train_vi_en)\n",
    "\n",
    "\n",
    "train_vi_en_iter = torchtext.data.BucketIterator(train_vi_en, batch_size=4, sort_key= lambda e: len(e.source) + len(e.target),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=4, sort_key= lambda e: len(e.source) + len(e.target),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "    \n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "    \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    '''\n",
    "    query: batch, seq1, d_k\n",
    "    key: batch, seq2, d_k\n",
    "    value: batch, seq2, embedding_size\n",
    "    mask: batch, 1, seq_2\n",
    "    '''\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_k\n",
    "        self.linears = clones(nn.Linear(d_model, d_k), 2)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key = [l(x) for l, x in zip(self.linears, (query, key))]\n",
    "        #query, key = batch, seq, d_k\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        #x: batch, seq_query, embedding_size\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(F.relu(self.w_1(x)))\n",
    "    \n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.sum() > 0 and len(mask) > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "    \n",
    "    \n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1))\n",
    "        return loss\n",
    "    \n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/ql819/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('../../machine_translation_attention/save_model/vi_to_en_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,  9794, 12230, 30385,  3967, 37037, 37221, 37221, 37221, 37221,\n",
      "         18247, 18247, 32363, 24118, 37221,  2496, 12230, 18247, 22832, 35480,\n",
      "         18247,  7971, 24118, 37221, 44315, 35785, 18247, 37221,  3963, 30835,\n",
      "         45696, 37221, 11409, 35150, 18247, 24973, 30835, 34219, 39120, 44471,\n",
      "         44266, 18247, 45696, 35150,  9794, 18194, 15957, 45696,  8545, 46928,\n",
      "         21895,  3963,  1104,  3430, 29435,  9084, 11409, 39120,  4641, 45696]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word.item() == end_symbol:\n",
    "            break\n",
    "    return ys\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source) + len(e.target),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "for batch in (validation_vi_en_iter):\n",
    "    \n",
    "    source, target = batch.source, batch.target\n",
    "    source_data, source_len = source[0], source[1]\n",
    "    target_data, target_len = target[0], target[1]\n",
    "\n",
    "    source_mask = (source_data != TEXT_vi.vocab.stoi['<pad>']).unsqueeze(1)\n",
    "    #source_mask: batch, 1, source_sen_len\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    out = greedy_decode(model, source_data, source_mask, \n",
    "                        max_len=60, start_symbol=TEXT_en.vocab.stoi[\"<sos>\"],\n",
    "                       end_symbol=TEXT_en.vocab.stoi[\"<eos>\"])\n",
    "    print(\"Translation:\", end=\"\\t\")\n",
    "    for i in range(1, out.size(1)):\n",
    "        sym = TGT.vocab.itos[out[0, i]]\n",
    "        if sym == \"</s>\": break\n",
    "        print(sym, end =\" \")\n",
    "    print()\n",
    "    print(\"Target:\", end=\"\\t\")\n",
    "    for i in range(1, batch.trg.size(0)):\n",
    "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
    "        if sym == \"</s>\": break\n",
    "        print(sym, end =\" \")\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bean_Search_Status_Record:\n",
    "    \n",
    "    def __init__(self, predict_word_index_list, sum_log_prob):\n",
    "        self.predict_word_index_list = predict_word_index_list\n",
    "        self.sum_log_prob = sum_log_prob\n",
    "        self.avg_log_prob = 0\n",
    "        \n",
    "    \n",
    "\n",
    "def test(model, data_iter, k=10):\n",
    "    model.eval()\n",
    "\n",
    "    path_name = '../eval/'+str(time.time()).replace('.','_')+'/'\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "    predict_file_name = path_name + 'predict.txt'\n",
    "    target_file_name = path_name + 'target_file_name.txt'\n",
    "\n",
    "    predict_file = open(predict_file_name, 'w')\n",
    "    target_file = open(target_file_name, 'w')\n",
    "\n",
    "\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        source_mask = (source_data != TEXT_vi.vocab.stoi['<pad>']).unsqueeze(1)\n",
    "        \n",
    "        memory = model.encode(source_data, source_mask)\n",
    "        \n",
    "        target_word = TEXT_en.vocab.stoi['<sos>']\n",
    "\n",
    "\n",
    "        right_whole_sentence_word_index = target_data[1: target_len[0].item()-1,0]\n",
    "        right_whole_sentence_word_index = list(right_whole_sentence_word_index.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        sequences = [Bean_Search_Status_Record(predict_word_index_list = [target_word], \n",
    "                                               sum_log_prob = 0.0)]\n",
    "        \n",
    "        t = 0\n",
    "        while (t < 100):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                record = sequences[i]\n",
    "                predict_word_index_list = record.predict_word_index_list\n",
    "                predict_word_index_list_tensor = torch.tensor(predict_word_index_list).view(1,-1).type_as(source_data)\n",
    "                sum_log_prob = record.sum_log_prob\n",
    "                last_word_index = predict_word_index_list[-1]\n",
    "                \n",
    "                if TEXT_en.vocab.stoi['<eos>'] != last_word_index:\n",
    "                \n",
    "                    out = model.decode(memory, source_mask, \n",
    "                                       Variable(predict_word_index_list_tensor), \n",
    "                                       Variable(subsequent_mask(predict_word_index_list_tensor.size(1))\n",
    "                                                .type_as(source_data)))\n",
    "                    prob = model.generator(out[:, -1])\n",
    "        \n",
    "                    k_prob_value_list, k_word_index_list = prob.topk(k,dim=1)\n",
    "                    k_prob_value_list = k_prob_value_list.cpu().detach().squeeze().numpy()\n",
    "                    k_word_index_list = k_word_index_list.cpu().squeeze().numpy()\n",
    "                    \n",
    "                    \n",
    "                    for prob_value, word_index in zip(k_prob_value_list, k_word_index_list):\n",
    "                        prob_value = float(prob_value)\n",
    "                        word_index = int(word_index)\n",
    "                        new_record = Bean_Search_Status_Record( predict_word_index_list+[word_index], sum_log_prob+prob_value)\n",
    "                        new_record.avg_log_prob = new_record.sum_log_prob/(len(new_record.predict_word_index_list) - 1)\n",
    "                        all_candidates.append(new_record)\n",
    "                else:\n",
    "                    all_candidates.append(record)\n",
    "                        \n",
    "            ordered = sorted(all_candidates, key = lambda r: r.sum_log_prob, reverse = True)\n",
    "            sequences = ordered[:k]\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "        final_record = sequences[0]\n",
    "        \n",
    "        \n",
    "        predict_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in final_record.predict_word_index_list[1:-1]]\n",
    "        right_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in right_whole_sentence_word_index]\n",
    "\n",
    "        predict_whole_sentence = ' '.join(predict_whole_sentence_word_index)\n",
    "        right_whole_sentence = ' '.join(right_whole_sentence_word_index)\n",
    "\n",
    "        predict_file.write(predict_whole_sentence.strip() + '\\n')\n",
    "        target_file.write(right_whole_sentence.strip() + '\\n')\n",
    "\n",
    "\n",
    "    predict_file.close()\n",
    "    target_file.close()\n",
    "\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file_name,target_file_name),shell=True,stdout=subprocess.PIPE)\n",
    "    result = str(result)\n",
    "    print(result)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    return get_blue_score(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4c62bb2bd93e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                              repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_vi_en_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-6c982782f61a>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, data_iter, k)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                        \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_word_index_list_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                                        Variable(subsequent_mask(predict_word_index_list_tensor.size(1))\n\u001b[0;32m---> 62\u001b[0;31m                                                 .type_as(source_data)))\n\u001b[0m\u001b[1;32m     63\u001b[0m                     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-26d6376b1a38>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, memory, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-26d6376b1a38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-26d6376b1a38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;34m\"Follow Figure 1 (right) for connections.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-26d6376b1a38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;34m\"Apply residual connection to any sublayer with the same size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-26d6376b1a38>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;34m\"Follow Figure 1 (right) for connections.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-gpu/py3.6.3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-26d6376b1a38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# 2) Apply attention on all the projected vectors in batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         x, self.attn = attention(query, key, value, mask=mask, \n\u001b[0;32m--> 196\u001b[0;31m                                  dropout=self.dropout)\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;31m#x: batch, seq_query, embedding_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-26d6376b1a38>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mp_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source) + len(e.target),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "s = test(model, validation_vi_en_iter, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2711, 5, 19, 17, 42, 1193, 770, 5, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.predict_word_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 35, 512])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.sum() > 0 and len(mask) > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "    \n",
    "    \n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        return loss.cpu().detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(size=len(TEXT_en.vocab.stoi), padding_idx=TEXT_en.vocab.stoi['<pad>'], smoothing=0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "compute_loss = SimpleLossCompute(model.generator, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss(out, target_true_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 21])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (t!=1).unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 21])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 1, 21])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.rand(32, 8, 21, 21).cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.masked_fill(mask == 0, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.nn.functional.softmax(scores, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]]],\n",
       "       device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0685, 0.0620, 0.0439, 0.0575, 0.0468, 0.0452, 0.0655, 0.0710, 0.0907,\n",
       "         0.0386, 0.0478, 0.0623, 0.0604, 0.0673, 0.0451, 0.0695, 0.0579, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0628, 0.0387, 0.0506, 0.0471, 0.0391, 0.0772, 0.0495, 0.0927, 0.0786,\n",
       "         0.0512, 0.0401, 0.0535, 0.0969, 0.0381, 0.0822, 0.0467, 0.0550, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0626, 0.0405, 0.0416, 0.0741, 0.0831, 0.0747, 0.0667, 0.0467, 0.0736,\n",
       "         0.0425, 0.0484, 0.0594, 0.0748, 0.0670, 0.0400, 0.0333, 0.0707, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0490, 0.0718, 0.0441, 0.0702, 0.0914, 0.0573, 0.0439, 0.0777, 0.0786,\n",
       "         0.0358, 0.0481, 0.0428, 0.0567, 0.0430, 0.0818, 0.0693, 0.0387, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0888, 0.0424, 0.0561, 0.0669, 0.0836, 0.0452, 0.0630, 0.0384, 0.0355,\n",
       "         0.0545, 0.0623, 0.0389, 0.0348, 0.0746, 0.0701, 0.0589, 0.0859, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0838, 0.0601, 0.0453, 0.0418, 0.0501, 0.0509, 0.0503, 0.0648, 0.0465,\n",
       "         0.0974, 0.0695, 0.0579, 0.0404, 0.0511, 0.0666, 0.0515, 0.0721, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0558, 0.0554, 0.0662, 0.0566, 0.0540, 0.0914, 0.0716, 0.0503, 0.0874,\n",
       "         0.0384, 0.0419, 0.0717, 0.0411, 0.0425, 0.0407, 0.0893, 0.0456, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0385, 0.0569, 0.0887, 0.0342, 0.0757, 0.0462, 0.0413, 0.0797, 0.0656,\n",
       "         0.0455, 0.0680, 0.0749, 0.0502, 0.0510, 0.0784, 0.0337, 0.0715, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0465, 0.0689, 0.0391, 0.0506, 0.0626, 0.0552, 0.0767, 0.0403, 0.0383,\n",
       "         0.0800, 0.0784, 0.0564, 0.0445, 0.0602, 0.0620, 0.0852, 0.0551, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0825, 0.0723, 0.0342, 0.0617, 0.0472, 0.0826, 0.0746, 0.0410, 0.0720,\n",
       "         0.0821, 0.0665, 0.0604, 0.0389, 0.0348, 0.0490, 0.0402, 0.0600, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0773, 0.0733, 0.0515, 0.0337, 0.0669, 0.0532, 0.0344, 0.0787, 0.0740,\n",
       "         0.0780, 0.0405, 0.0765, 0.0544, 0.0590, 0.0559, 0.0504, 0.0424, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0466, 0.0357, 0.0750, 0.0408, 0.0510, 0.0817, 0.0603, 0.0765, 0.0366,\n",
       "         0.0503, 0.0813, 0.0787, 0.0420, 0.0457, 0.0635, 0.0668, 0.0673, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0703, 0.0557, 0.0458, 0.0404, 0.0705, 0.0337, 0.0674, 0.0610, 0.0885,\n",
       "         0.0679, 0.0492, 0.0776, 0.0528, 0.0496, 0.0653, 0.0558, 0.0485, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0769, 0.0855, 0.0400, 0.0467, 0.0425, 0.0538, 0.0720, 0.0655, 0.0472,\n",
       "         0.0511, 0.0387, 0.0665, 0.0786, 0.0387, 0.0727, 0.0870, 0.0365, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0859, 0.0582, 0.0351, 0.0810, 0.0376, 0.0584, 0.0463, 0.0776, 0.0390,\n",
       "         0.0672, 0.0487, 0.0707, 0.0354, 0.0819, 0.0378, 0.0796, 0.0598, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0406, 0.0524, 0.0391, 0.0836, 0.0428, 0.0518, 0.0881, 0.0335, 0.0791,\n",
       "         0.0820, 0.0843, 0.0398, 0.0360, 0.0383, 0.0669, 0.0879, 0.0538, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0505, 0.0438, 0.0333, 0.0693, 0.0778, 0.0340, 0.0454, 0.0607, 0.0725,\n",
       "         0.0658, 0.0360, 0.0353, 0.0793, 0.0541, 0.0883, 0.0699, 0.0839, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0748, 0.0526, 0.0626, 0.0823, 0.0495, 0.0907, 0.0530, 0.0799, 0.0747,\n",
       "         0.0457, 0.0498, 0.0368, 0.0572, 0.0350, 0.0378, 0.0836, 0.0339, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0486, 0.0810, 0.0712, 0.0762, 0.0496, 0.0658, 0.0501, 0.0436, 0.0401,\n",
       "         0.0896, 0.0417, 0.0513, 0.0605, 0.0478, 0.0438, 0.0434, 0.0955, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0555, 0.0612, 0.0420, 0.0499, 0.0865, 0.0386, 0.0458, 0.0594, 0.0561,\n",
       "         0.0600, 0.0590, 0.0503, 0.0783, 0.0407, 0.1006, 0.0496, 0.0666, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0618, 0.0751, 0.0892, 0.0461, 0.0894, 0.0593, 0.0490, 0.0363, 0.0864,\n",
       "         0.0807, 0.0528, 0.0433, 0.0367, 0.0593, 0.0517, 0.0378, 0.0450, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[4,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        #x: batch, target_sentence_len, target_vocab_size\n",
    "        #y: batch, target_sentence_len\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data[0] * norm\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        '''\n",
    "        x: batch * target_sentence_len, target_vob_size\n",
    "        target: batch * target_sentence_len\n",
    "        '''\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2*3,10, requires_grad=True)\n",
    "y = torch.tensor([4,3,1,7,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0125, 0.0000, 0.0125, 0.0125, 0.9000, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "         0.0125],\n",
       "        [0.0125, 0.0000, 0.0125, 0.9000, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "         0.0125],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0125, 0.0000, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.9000, 0.0125,\n",
       "         0.0125],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dist = x.data.clone()\n",
    "true_dist.fill_(0.1 / (10 - 2)).int()\n",
    "true_dist.scatter_(1, y.data.unsqueeze(1), 0.9)\n",
    "true_dist[:, 1] = 0\n",
    "mask = torch.nonzero(y.data == 1)\n",
    "true_dist.index_fill_(0, mask.squeeze(), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.KLDivLoss(reduction='sum')\n",
    "loss = a(x[:],true_dist[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0125,  0.0000, -0.0125, -0.0125, -0.9000, -0.0125, -0.0125, -0.0125,\n",
       "         -0.0125, -0.0125],\n",
       "        [-0.0125,  0.0000, -0.0125, -0.9000, -0.0125, -0.0125, -0.0125, -0.0125,\n",
       "         -0.0125, -0.0125],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.0250,  0.0000, -0.0250, -0.0250, -0.0250, -0.0250, -0.0250, -1.8000,\n",
       "         -0.0250, -0.0250],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  149,   668,   567,  ...,  1590,     3,     1],\n",
       "          [ 1804, 14556,    67,  ...,    80,  2659,     1],\n",
       "          [ 1804,  6614,   487,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [   35,     7,     6,  ..., 20397,     3,     1],\n",
       "          [    4,   193,    12,  ...,     1,     1,     1],\n",
       "          [    4,   181,     5,  ...,     3,     1,     1]],\n",
       " \n",
       "         [[  149,   668,   567,  ...,  1590,     3,     1],\n",
       "          [ 1804, 14556,    67,  ...,    80,  2659,     1],\n",
       "          [ 1804,  6614,   487,  ...,     1,     1,     1],\n",
       "          ...,\n",
       "          [   35,     7,     6,  ..., 20397,     3,     1],\n",
       "          [    4,   193,    12,  ...,     1,     1,     1],\n",
       "          [    4,   181,     5,  ...,     3,     1,     1]]], device='cuda:0'),\n",
       " tensor([20, 20, 18, 20, 17, 14, 19, 17, 17, 21, 18, 17, 21, 19, 18, 19, 21, 19,\n",
       "         18, 15, 19, 19, 16, 17, 16, 19, 18, 16, 19, 20, 17, 19],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, sys, os\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "import csv\n",
    "import re, random, string, subprocess, time\n",
    "\n",
    "\n",
    "TEXT_vi = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = True, tokenize= lambda t:t.split(),\n",
    "                                        include_lengths=True)\n",
    "TEXT_en = torchtext.data.ReversibleField(sequential=True, use_vocab=True, batch_first = False, tokenize= lambda t:t.split(),\n",
    "                              lower=True, init_token='<sos>', eos_token='<eos>',include_lengths=True)\n",
    "train_vi_en = torchtext.data.TabularDataset('/home/ql819/text_data/train_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "validation_vi_en = torchtext.data.TabularDataset('/home/ql819/text_data/dev_vi_en.csv', format='csv', \n",
    "                             fields=[('source',TEXT_vi),('target',TEXT_en)])\n",
    "\n",
    "\n",
    "TEXT_vi.build_vocab(train_vi_en, min_freq=3)\n",
    "TEXT_en.build_vocab(train_vi_en, min_freq=3)\n",
    "\n",
    "train_vi_en_iter = torchtext.data.BucketIterator(train_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "validation_vi_en_iter = torchtext.data.BucketIterator(validation_vi_en, batch_size=1, sort_key= lambda e: len(e.source),\n",
    "                             repeat = False, sort_within_batch=True, shuffle=True, device=torch.device(0))\n",
    "\n",
    "\n",
    "class GRU_Decoder_With_Attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_vocab, input_size, hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = 1\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(self.num_vocab, self.input_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size= self.hidden_size, input_size= self.input_size + 1 * self.hidden_size, \n",
    "                                  num_layers= self.num_layers)\n",
    "        \n",
    "        self.calcu_weight_1  = torch.nn.Linear(2*self.hidden_size, hidden_size)\n",
    "        self.calcu_weight_2  = torch.nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        self.init_weight = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "        self.linear_vob = torch.nn.Linear(self.hidden_size, self.num_vocab)\n",
    "        \n",
    "    def forward(self, input_word_index, hidden_vector, encoder_memory, is_init = False):\n",
    "        #input_word_index: [num]\n",
    "        #hidden_vector: 1, 1, hidden_size\n",
    "        #encoder_memory: source_sen_len , 1 * hidden_size\n",
    "        \n",
    "        if hidden_vector.shape[0] != self.num_layers or hidden_vector.shape[2] != self.hidden_size:\n",
    "            raise ValueError('The size of hidden_vector is not correct, expect '+str((self.num_layers, self.hidden_size))\\\n",
    "                            + ', actually get ' + str(hidden_vector.shape))\n",
    "        \n",
    "        if is_init:\n",
    "            hidden_vector = torch.tanh(self.init_weight(hidden_vector))\n",
    "        \n",
    "        \n",
    "        n_hidden_vector = torch.stack([hidden_vector.squeeze()]*encoder_memory.shape[0],dim=0)\n",
    "        com_n_h_memory = torch.cat([n_hidden_vector, encoder_memory], dim =1)\n",
    "        com_n_h_temp = torch.tanh(self.calcu_weight_1(com_n_h_memory))\n",
    "        \n",
    "        \n",
    "        weight_vector = self.calcu_weight_2(com_n_h_temp)\n",
    "        weight_vector =  torch.nn.functional.softmax(weight_vector, dim=0)\n",
    "        #weight_vector: source_sen_len * 1\n",
    "        \n",
    "        \n",
    "        convect_vector = torch.mm(weight_vector.transpose(1,0), encoder_memory)\n",
    "        #convect_vector: 1 , 2 * hidden_size\n",
    "        \n",
    "        \n",
    "        input_vector = self.embedding_layer(input_word_index).view(1,1,-1)\n",
    "        \n",
    "        \n",
    "        input_vector = torch.cat([convect_vector.unsqueeze(0), input_vector], dim=2)\n",
    "        \n",
    "        \n",
    "        output, h_t = self.gru(input_vector,hidden_vector)\n",
    "        output = output.view(1, self.hidden_size)\n",
    "        \n",
    "        \n",
    "        prob = self.linear_vob(output)\n",
    "        #prob 1, vob_size\n",
    "        \n",
    "        prob = torch.nn.functional.log_softmax(prob, dim=1)\n",
    "        \n",
    "        \n",
    "        return prob, h_t\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, src_embed, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_embed = src_embed\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        x = self.src_embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "\n",
    "    \n",
    "def attention(query, key, value, dropout=None):\n",
    "    '''\n",
    "    query: batch, seq1, d_k\n",
    "    key: batch, seq2, d_k\n",
    "    value: batch, seq2, embedding_size\n",
    "    mask: batch, 1, seq_2\n",
    "    '''\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_k\n",
    "        self.linears = clones(nn.Linear(d_model, d_k), 2)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        \"Implements Figure 2\"\n",
    "        \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key = [l(x) for l, x in zip(self.linears, (query, key))]\n",
    "        #query, key = batch, seq, d_k\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value,\n",
    "                                 dropout=self.dropout)\n",
    "        #x: batch, seq_query, embedding_size\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(F.relu(self.w_1(x)))\n",
    "    \n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "    \n",
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_k=64, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(d_k, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), nn.Sequential(Embeddings(d_model, src_vocab), c(position)), N)\n",
    "    decoder = GRU_Decoder_With_Attention(num_vocab = tgt_vocab, input_size = d_model, hidden_size = d_model)\n",
    "    for p in encoder.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "\n",
    "def train(encoder, decoder, optimizer, data_iter, teacher_forcing_ratio, batch_size = 64):\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    count = 0\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output = encoder(source_data)\n",
    "        #all_output: 1, source_len, embedding_size\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "\n",
    "        output = all_output[:,0]\n",
    "        target_word_list = target_data.squeeze()\n",
    "        target_word = torch.tensor([TEXT_en.vocab.stoi['<sos>']]).cuda(0)\n",
    "\n",
    "        h_t = output[0,:]\n",
    "        h_t = h_t.view([1,1,-1])\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "        for word_index in range(1, target_len[0].item()):\n",
    "            prob, h_t = decoder(target_word, h_t, output, is_init)\n",
    "            is_init = False\n",
    "            if use_teacher_forcing:\n",
    "                target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, target_word)\n",
    "            else:\n",
    "                right_target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, right_target_word)\n",
    "                predict_target_word_index = prob.topk(1)[1].item()\n",
    "\n",
    "                if TEXT_en.vocab.stoi['<eos>'] == predict_target_word_index:\n",
    "                    break\n",
    "                else:\n",
    "                    target_word = torch.tensor([predict_target_word_index]).cuda(0)\n",
    "                    \n",
    "        count += 1\n",
    "        if count % batch_size == 0:\n",
    "            \n",
    "            loss = loss/batch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            count = 0\n",
    "            loss = 0\n",
    "        \n",
    "        \n",
    "    if count % batch_size != 0:\n",
    "        loss = loss/count\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "class Bean_Search_Status_Record:\n",
    "    \n",
    "    def __init__(self, h_t, predict_word_index_list, sum_log_prob):\n",
    "        self.h_t = h_t\n",
    "        self.predict_word_index_list = predict_word_index_list\n",
    "        self.sum_log_prob = sum_log_prob\n",
    "        self.avg_log_prob = 0\n",
    "        \n",
    "    \n",
    "\n",
    "def test(encoder, decoder, data_iter, k=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    path_name = '../eval/'+str(time.time()).replace('.','_')+'/'\n",
    "    os.mkdir(path_name)\n",
    "\n",
    "    predict_file_name = path_name + 'predict.txt'\n",
    "    target_file_name = path_name + 'target_file_name.txt'\n",
    "\n",
    "    predict_file = open(predict_file_name, 'w')\n",
    "    target_file = open(target_file_name, 'w')\n",
    "\n",
    "\n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output = encoder(source_data)\n",
    "        output = all_output[:,0]\n",
    "        \n",
    "        target_word = torch.tensor([TEXT_en.vocab.stoi['<sos>']]).cuda(0)\n",
    "\n",
    "        h_t = output[0,:]\n",
    "        h_t = h_t.view([1,1,-1])\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "\n",
    "        right_whole_sentence_word_index = target_data[1: target_len[0].item()-1,0]\n",
    "        right_whole_sentence_word_index = list(right_whole_sentence_word_index.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        sequences = [Bean_Search_Status_Record(h_t, predict_word_index_list = [target_word], \n",
    "                                               sum_log_prob = 0.0)]\n",
    "        \n",
    "        t = 0\n",
    "        while (t < 100):\n",
    "            all_candidates = []\n",
    "            for i in range(len(sequences)):\n",
    "                record = sequences[i]\n",
    "                h_t = record.h_t\n",
    "                predict_word_index_list = record.predict_word_index_list\n",
    "                sum_log_prob = record.sum_log_prob\n",
    "                target_word = predict_word_index_list[-1]\n",
    "                \n",
    "                if TEXT_en.vocab.stoi['<eos>'] != target_word:\n",
    "                \n",
    "                    prob, h_t = decoder(torch.tensor([target_word]).cuda(0), h_t, output, is_init)\n",
    "\n",
    "                    k_prob_value_list, k_word_index_list = prob.topk(k,dim=1)\n",
    "                    k_prob_value_list = k_prob_value_list.cpu().detach().squeeze().numpy()\n",
    "                    k_word_index_list = k_word_index_list.cpu().squeeze().numpy()\n",
    "                    \n",
    "                    \n",
    "                    for prob_value, word_index in zip(k_prob_value_list, k_word_index_list):\n",
    "                        prob_value = float(prob_value)\n",
    "                        word_index = int(word_index)\n",
    "                        new_record = Bean_Search_Status_Record(h_t, predict_word_index_list+[word_index], sum_log_prob+prob_value)\n",
    "                        new_record.avg_log_prob = new_record.sum_log_prob/(len(new_record.predict_word_index_list) - 1)\n",
    "                        all_candidates.append(new_record)\n",
    "                else:\n",
    "                    all_candidates.append(record)\n",
    "            is_init = False\n",
    "                        \n",
    "            ordered = sorted(all_candidates, key = lambda r: r.sum_log_prob, reverse = True)\n",
    "            sequences = ordered[:k]\n",
    "            \n",
    "            t += 1\n",
    "        final_record = sequences[0]\n",
    "        \n",
    "        predict_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in final_record.predict_word_index_list[1:-1]]\n",
    "        right_whole_sentence_word_index = [TEXT_en.vocab.itos[temp_index] for temp_index in right_whole_sentence_word_index]\n",
    "\n",
    "        predict_whole_sentence = ' '.join(predict_whole_sentence_word_index)\n",
    "        right_whole_sentence = ' '.join(right_whole_sentence_word_index)\n",
    "\n",
    "        predict_file.write(predict_whole_sentence.strip() + '\\n')\n",
    "        target_file.write(right_whole_sentence.strip() + '\\n')\n",
    "\n",
    "\n",
    "    predict_file.close()\n",
    "    target_file.close()\n",
    "\n",
    "    result = subprocess.run('cat {} | sacrebleu {}'.format(predict_file_name,target_file_name),shell=True,stdout=subprocess.PIPE)\n",
    "    result = str(result)\n",
    "    print(result)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    return get_blue_score(result)\n",
    "\n",
    "\n",
    "def get_blue_score(s):\n",
    "    a = re.search(r'13a\\+version\\.1\\.2\\.12 = ([0-9.]+)',s)\n",
    "    return float(a.group(1))\n",
    "\n",
    "\n",
    "\n",
    "def parameters_list_change_grad(encoder, decoder):\n",
    "    para_list = []\n",
    "    for name, data in list(encoder.named_parameters()):\n",
    "        if 'src_embed' in name:\n",
    "            data.requires_grad = False\n",
    "        else:\n",
    "            para_list.append(data)\n",
    "            \n",
    "    for name, data in list(decoder.named_parameters()):\n",
    "        if 'embedding' in name:\n",
    "            data.requires_grad = False\n",
    "        else:\n",
    "            para_list.append(data)\n",
    "    return para_list        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder,decoder = make_model(src_vocab=len(TEXT_vi.vocab.stoi), tgt_vocab=len(TEXT_en.vocab.stoi), N=6, \n",
    "               d_model=512, d_k=64, dropout=0.1)\n",
    "\n",
    "encoder = encoder.cuda(0)\n",
    "decoder = decoder.cuda(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, optimizer, data_iter, teacher_forcing_ratio, batch_size = 64):\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    count = 0\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    for batch in data_iter:\n",
    "        \n",
    "        \n",
    "        source, target = batch.source, batch.target\n",
    "        \n",
    "\n",
    "        source_data,source_len = source[0], source[1]\n",
    "        target_data,target_len = target[0], target[1]\n",
    "        \n",
    "        all_output = encoder(source_data)\n",
    "        #all_output: 1, source_len, embedding_size\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "\n",
    "        output = all_output[:,0]\n",
    "        target_word_list = target_data.squeeze()\n",
    "        target_word = torch.tensor([TEXT_en.vocab.stoi['<sos>']]).cuda(0)\n",
    "\n",
    "        h_t = output[0,:]\n",
    "        h_t = h_t.view([1,1,-1])\n",
    "\n",
    "        is_init = True\n",
    "\n",
    "        for word_index in range(1, target_len[0].item()):\n",
    "            prob, h_t = decoder(target_word, h_t, output, is_init)\n",
    "            is_init = False\n",
    "            if use_teacher_forcing:\n",
    "                target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, target_word)\n",
    "            else:\n",
    "                right_target_word = target_word_list[[word_index]]\n",
    "                loss += torch.nn.functional.nll_loss(prob, right_target_word)\n",
    "                predict_target_word_index = prob.topk(1)[1].item()\n",
    "\n",
    "                if TEXT_en.vocab.stoi['<eos>'] == predict_target_word_index:\n",
    "                    break\n",
    "                else:\n",
    "                    target_word = torch.tensor([predict_target_word_index]).cuda(0)\n",
    "                    \n",
    "        count += 1\n",
    "        if count % batch_size == 0:\n",
    "            \n",
    "            loss = loss/batch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            count = 0\n",
    "            loss = 0\n",
    "        \n",
    "        \n",
    "    if count % batch_size != 0:\n",
    "        loss = loss/count\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
